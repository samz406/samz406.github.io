<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sanmuzi.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Apache Doris 技术调研报告">
<meta property="og:type" content="article">
<meta property="og:title" content="Apache Doris 技术调研报告">
<meta property="og:url" content="http://www.sanmuzi.com/2025/08/01/Apache-Doris/index.html">
<meta property="og:site_name" content="一子三木">
<meta property="og:description" content="Apache Doris 技术调研报告">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-08-01T02:06:42.000Z">
<meta property="article:modified_time" content="2025-08-15T12:01:09.329Z">
<meta property="article:author" content="爱妙妙爱生活">
<meta property="article:tag" content="架构">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://www.sanmuzi.com/2025/08/01/Apache-Doris/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Apache Doris 技术调研报告 | 一子三木</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">一子三木</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">所看 所学 所思</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://www.sanmuzi.com/2025/08/01/Apache-Doris/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="爱妙妙爱生活">
      <meta itemprop="description" content="日拱一卒，功不唐捐">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一子三木">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Apache Doris 技术调研报告
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-08-01 10:06:42" itemprop="dateCreated datePublished" datetime="2025-08-01T10:06:42+08:00">2025-08-01</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A0%94%E7%A9%B6%E6%8A%A5%E5%91%8A/" itemprop="url" rel="index"><span itemprop="name">研究报告</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p> Apache Doris 技术调研报告</p>
<span id="more"></span>

<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Apache Doris 是一款面向实时分析的现代化 MPP 数据仓库，具有亚秒级查询性能和高并发支持。Doris 最初由百度开发（原名 Palo），2017 年开源并于 2018 年捐赠给 Apache 基金会，2022 年毕业成为顶级项目。Doris 采用<strong>存储与计算一体化</strong>架构，部署运维简洁，仅需两类进程（前端 FE 和后端 BE），无外部依赖，可线性扩展至百台节点、PB 级数据规模。凭借优秀的性能和易用性，Doris 已在包括字节跳动、百度、腾讯、网易等在内的5000多家公司生产部署，并广泛应用于金融、零售、电信、能源、制造、医疗等行业。常见使用场景涵盖实时报表和决策分析、即席多维查询、用户行为与画像分析、湖仓一体查询加速、日志与指标即席分析等。</p>
<p>Doris 提供高度兼容 MySQL 的 SQL 接口和生态支持，内置向量化执行引擎和 MPP 并行架构，可在大数据集上实现子秒级查询延迟。其存储层采用列式存储和多级索引技术，支持高压缩比和快速过滤。Doris 还具备强大的数据导入能力（批量导入与实时流式导入）、物化视图和多种索引加速、主键模型实时更新、高并发优化策略，以及与 Flink、Spark、Hive、Kafka 等主流大数据生态的良好集成。本报告将系统阐述 Apache Doris 的整体架构设计、存储与查询引擎原理、数据导入导出机制、SQL 特性、性能优化策略、生态集成、同类产品对比、企业实践案例、部署与运维能力以及社区生态与未来路线。内容基于官方文档、权威技术文章和实际案例调研，旨在为企业技术选型和分享提供全面参考。</p>
<h2 id="1-Doris-整体架构设计"><a href="#1-Doris-整体架构设计" class="headerlink" title="1. Doris 整体架构设计"></a>1. Doris 整体架构设计</h2><p><strong>Apache Doris 的整体架构</strong>采用存算一体模式，只有前端节点（Frontend, FE）和后端节点（Backend, BE）两种进程角色。这种高度集成的架构简化了部署和运维，不依赖外部协调组件，降低了分布式系统的复杂性。下图展示了 Doris 集群架构示意：</p>
<h3 id="1-1-Frontend-FE-前端架构"><a href="#1-1-Frontend-FE-前端架构" class="headerlink" title="1.1 Frontend (FE) 前端架构"></a>1.1 Frontend (FE) 前端架构</h3><p>Frontend 节点主要承担<strong>元数据管理</strong>和<strong>查询调度</strong>等职能。Doris 的 FE 由 Java 编写，支持多实例部署用于高可用和读扩展。在元数据方面，FE 存储维护整个集群的元数据，包括数据库和表的 Schema 定义、分区和分片（Tablet）分布信息、用户权限以及各种作业(Job)状态等。多个 FE 通过 Berkeley DB Java Edition (BDB JE) 组成元数据复制组，实现元数据的强一致复制与选主。FE 分为三种角色：<strong>Master</strong>（主节点）、<strong>Follower</strong>（从节点）和<strong>Observer</strong>（观察节点）。Master 负责元数据读写和对外提供服务，并通过复制协议将元数据变更同步给 Follower/Observer。当 Master 故障时，Follower 可自动选举为新 Master，保证服务连续。Observer 节点只接收元数据更新但不参与选主，用于水平扩展查询的并发处理能力。</p>
<p>在查询路径上，FE 充当<strong>SQL 请求入口和调度者</strong>。它接收客户端发送的查询（使用 MySQL 协议），执行词法语法和语义解析，将 SQL 转换为逻辑执行计划和分布式物理计划。FE 基于全局元数据和优化规则对执行计划做优化，如选择连接顺序、过滤下推等，然后将计划切分为多个<strong>执行片段</strong>（Fragment），分别分配到对应的 BE 节点上执行。FE 负责协调各 BE 之间的数据交换（如 shuffle 分区）和最终结果合并。查询执行过程中，FE 持续监控各节点执行状态，必要时进行重试或 Failover。执行完成后，FE 收集各 BE 返回的结果并汇总为最终结果返回给客户端。总的来说，FE 扮演了 Doris 的“大脑”角色：既维护全局元数据一致性，又负责将用户请求翻译为并行任务并调度执行。</p>
<p>此外，FE 还承担<strong>集群管理和任务调度</strong>职能。例如，添加或下线 BE 节点、平衡数据分布、协调副本复制与恢复、以及执行 DDL 操作（建表、修改 Schema 等）都由 FE 处理。Doris 内置了<strong>作业调度模块</strong> (Job Scheduler)，支持定时执行某些导入或维护任务。该模块允许用户通过 <code>CREATE JOB</code> 定义周期性任务，FE 会在后台按照计划触发作业（比如定时增量导入、定期清理历史数据等），减少对外部调度系统的依赖。在 2.1 版本引入 Job Scheduler 后，Doris 可实现精确到秒级的自主调度，保障数据定时导入的一致性和完整性，并提供高可用的失败恢复能力。总之，Frontend 提供了整个 Doris 集群的入口、控制与管理功能，其无共享的分布式设计保证了高可用和线性扩展能力。</p>
<h3 id="1-2-Backend-BE-后端架构"><a href="#1-2-Backend-BE-后端架构" class="headerlink" title="1.2 Backend (BE) 后端架构"></a>1.2 Backend (BE) 后端架构</h3><p>Backend 节点主要负责<strong>数据存储</strong>和<strong>查询执行</strong>，由 C++ 实现，以充分发挥性能。每个 BE 管理本地存储上的一部分数据分片（Tablet），并执行来自 FE 下发的查询计划片段。<strong>存储方面</strong>，BE 承担实际的数据读写：它将数据按列存储于磁盘文件中，并负责将内存中的更新批次刷盘、合并小文件等操作。Doris 采用分区 (Partition)、分桶 (Bucket) 的分布策略将数据拆分成多个 <strong>Tablet</strong>（数据分片），每个 Tablet 是存储层管理的最小单位。在物理上，Tablet 下又包含若干按版本序列组织的 <strong>Rowset</strong>，Rowset 由一个或多个 Segment 文件组成，是数据增量导入的基本存储文件。BE 负责将新的数据写入内存 <strong>MemTable</strong>，经过排序和索引构建后以 Segment 刷写成新的 Rowset 文件。随着不断导入，BE 后台会异步执行 <strong>Compaction 合并</strong>任务，将多个小的 Rowset 合并为更大的文件、并淘汰过期版本，以降低查询扫描开销。因此，在 BE 节点本地，Tablet -&gt; Rowset -&gt; Segment 构成了递进的存储组织结构，用于管理增量数据版本和物理文件。</p>
<p><strong>查询方面</strong>，Backend 是实际执行算子的工作进程。FE 下发的物理计划会被分解为多个片段，每个片段包含一系列算子（Scan、Filter、Join、Agg等）的执行流水线。不同 BE 上的相应片段通过 Exchange 算子进行数据分发和交换（如按键哈希 shuffle）。在执行时，每个 BE 将扫描其本地存储中涉及的 Tablet 分片数据，应用谓词下推进行<strong>本地过滤</strong>，然后按照计划执行后续算子并与其他节点交换数据。Doris BE 利用 C++ 的高效内存处理和向量化算子来提升执行性能。每个 BE 都是<strong>无共享</strong>的：它们彼此对等，既存储自己的数据又执行部分查询，没有中心节点，从而具备良好的横向扩展能力和容错性。当 BE 节点故障时，其上数据有其他副本可用，查询能够绕过故障节点继续完成，待故障节点恢复或被替换后，FE 会自动触发数据重平衡将缺失副本补齐。</p>
<p><strong>模块组成方面</strong>，BE 内部可以进一步划分为存储引擎模块、执行引擎模块和网络通信模块等。存储引擎模块负责 Tablet 管理、数据文件读写、索引维护、Compaction、快照等；执行引擎模块实现各种查询算子（如 ScanNode、JoinNode、AggNode）的逻辑以及向量化执行；网络模块负责节点间数据的序列化传输和 RPC 通信。BE 还嵌入了一个<strong>Broker</strong>组件用于访问外部存储系统文件（如 HDFS、S3），在 Broker Load 场景下充当中转。值得注意的是，StarRocks（Doris 的衍生分支）和 Doris 在存储层实现上非常相似：两者均以 Tablet 为最小逻辑分片、Rowset 为增量数据集合并通过异步 Compaction 形成大型 Segment 文件，扫描时按 Segment 级顺序扫描。因此 Doris BE 可看作一个自带存储管理的并行查询执行节点，负责具体的数据处理和计算。</p>
<h3 id="1-3-任务调度与容错机制"><a href="#1-3-任务调度与容错机制" class="headerlink" title="1.3 任务调度与容错机制"></a>1.3 任务调度与容错机制</h3><p>Doris 集群的高性能运行离不开高效的任务调度机制和完备的容错策略。<strong>查询调度</strong>由 FE 根据全局视图完成：FE 知晓每个 Tablet 的副本分布，能够将查询计划片段尽量派发到数据本地的 BE 执行，减少网络开销。同时 FE 会在不同 BE 之间均衡负载，避免将过多任务压在单个节点上影响并发性能。Doris 3.0 对查询任务拆分和调度进行了优化：采用更加细粒度的片段切分和<strong>一致性哈希调度算法</strong>，确保所有节点受理的任务数量均衡，避免热点。对于包含海量小文件或分区的查询，Doris 3.0 引入<strong>异步批量调度</strong>策略来分阶段获取文件分片，从而将 FE 的调度开销降低一个数量级（例如将百万小文件查询延迟从100秒降至10秒）。这些改进使 Doris 在复杂查询场景下依然能高效利用集群资源并保持低延迟。</p>
<p>在<strong>后台任务调度</strong>方面，Doris FE 内置多种系统任务：如数据导入、Compaction、复制重平衡、Schema Change 等。FE 通过调度线程周期性检查需要执行的任务队列，并将具体任务指派给合适的 BE 节点执行。以 <strong>Compaction</strong> 为例：每个 BE 自主决定触发小文件合并，但 FE 也维护整个集群的 compaction 状态，必要时可以下发 compaction 指令确保关键 Tablet 及时合并。在导入流程中，FE 会将导入任务拆分为子任务（按分区/分片划分）并分发到各 BE，并跟踪每个子任务执行结果，以实现全局事务的提交或回滚。此外 Doris 2.1 的 Job Scheduler 提供<strong>精确定时</strong>的任务触发机制，保证定时作业（如每晚汇总、定期同步外部数据等）按计划执行。</p>
<p><strong>故障容错</strong>上，Doris 采用多副本和仲裁协议保证数据可靠与服务可用。每个数据 Tablet 通常有3个副本分散存储在不同 BE 上，写入采用过半确认策略，只要多数副本完成写入即认为成功，从而容忍少数节点故障。查询时，如果某副本不可达则自动从其他副本读取。FE 多节点部署使元数据服务具备故障切换能力，当 Master FE 异常中止时，存活的 Follower 会在几秒内完成选举成为新 Master，客户端也可通过配置多个 FE 地址实现透明重连。对于 BE 故障，FE 在检测到心跳超时后会将其标记为不可用并启动<strong>副本重建</strong>流程：在空闲节点上自动补充丢失的数据副本，使数据冗余度恢复到配置水平。Doris 支持同城和异地灾备部署，可通过创建双集群的主从模式来实现跨机房容灾。典型做法是定期使用 Doris 提供的 <strong>Backup/Restore</strong> 机制，将全量或增量数据快照备份到远端存储，然后在灾备集群上执行恢复。Doris 的备份可以基于快照保证全量数据一致性，并支持按分区增量备份以降低数据传输量。在需要迁移或升级集群时，也可采用“新节点加入、旧节点退出”的方式无缝过渡，Doris 会自动在新旧节点之间均衡数据，实现平滑迁移。通过这些机制，Doris 能确保在部分节点失效或重大变更场景下仍保持数据完整和服务连续。</p>
<p>综上，Apache Doris 简洁的两层架构在保证高性能的同时，充分考虑了分布式系统的可靠和弹性特性。FE/BE 的解耦让计算与存储协同但职责清晰，配合多副本和自动调度机制，实现了<strong>高可用、高可靠、易扩展</strong>的数据分析引擎架构。</p>
<h2 id="2-列式存储结构与数据组织"><a href="#2-列式存储结构与数据组织" class="headerlink" title="2. 列式存储结构与数据组织"></a>2. 列式存储结构与数据组织</h2><p>Doris 在存储层采用列式存储（Columnar Storage）并配以分片管理和多级索引，加速大规模数据的读取和分析。本节将介绍 Doris 的数据组织单位、列存文件格式、压缩编码和索引机制等核心设计。</p>
<h3 id="2-1-数据分片与目录组织"><a href="#2-1-数据分片与目录组织" class="headerlink" title="2.1 数据分片与目录组织"></a>2.1 数据分片与目录组织</h3><p><strong>分区 (Partition)</strong> 与 <strong>Tablet</strong> 是 Doris 数据切分的关键概念。用户创建表时通常按时间等字段定义分区，每个分区再按分桶键散列切分成若干 <strong>Bucket</strong>，每个 Bucket 对应一个 <strong>Tablet</strong>。因此，<strong>Tablet</strong> 可视为 Doris 存储的最小逻辑分片，一个表往往包含多个分区，每个分区下有多个 Tablet。集群中 Tablet 均匀分布在各 BE，且每个 Tablet 有多副本存储于不同 BE，实现水平拆分和高可用。<strong>Tablet 目录结构</strong>在 BE 本地体现为：每个 Tablet 有自己独立的存储目录（以 tablet_id 命名），目录下存放该 Tablet 的数据文件。Tablet 之间的数据相互独立，不同表/分区的 Tablet 也相互隔离。这种设计便于在节点间迁移或恢复单个 Tablet 的数据。</p>
<p>一个 <strong>Tablet</strong> 由一系列按照版本顺序排列的<strong>Rowset</strong>组成。每次导入（无论 INSERT 还是批量导入）都会为每个目标 Tablet 生成一个新的 Rowset，Rowset 内包含该批次导入的数据。Rowset 名称带有版本号，如 [7-7]、[8-8] 表示该 Rowset 覆盖的版本区间（这里为单一版本）。多个连续版本的 Rowset 经过 <strong>Compaction</strong> 合并后，会生成一个新的较大 Rowset，版本号区间覆盖合并前所有版本，如将 [6-6]、[7-7]、[8-8] 合并得到 [6-8]。旧的细粒度 Rowset 则标记删除或归档。通过这种多版本链，Doris 支持<strong>多版本并发读取</strong>和<strong>快照隔离</strong>：正在进行的查询可以读取旧版本数据，而导入可以生成新版本，待查询提交后再切换版本。这类似于 <strong>LSM-Tree</strong> 的思想，以追加的方式写入新数据，再逐步合并老数据。每个 Rowset 由若干 <strong>Segment 文件</strong>组成，一个 Segment 通常为列式存储的物理文件，默认为 256 MB 大小。Segment 文件以 <code>&#123;rowset_id&#125;_&#123;segment_id&#125;.dat</code> 命名保存在 Tablet 目录下。如果一个 Rowset 过大（超出单个 Segment 上限），会被划分为多个 Segment 文件。</p>
<p>Doris 将上述关系总结如下：<strong>表 -&gt; 分区 -&gt; 分桶 (Bucket) -&gt; Tablet -&gt; Rowset -&gt; Segment</strong>。这种层次结构充分利用分区裁剪、分片并行和增量版本，实现了对海量数据的有效组织和管理。下图展示了 Doris 数据组织的关系：</p>
<h3 id="2-2-列式存储与压缩编码"><a href="#2-2-列式存储与压缩编码" class="headerlink" title="2.2 列式存储与压缩编码"></a>2.2 列式存储与压缩编码</h3><p>Doris 使用列存格式存储数据，以减少 I/O 和加速矢量化处理。每个 Segment 文件中，数据按照列进行存放，即各列的数据各自顺序存储，并分块组织为<strong>数据页 (Data Page)<strong>。Doris 默认的数据页大小约为 64KB，每列的数据可能由多个页组成。</strong>列式存储的优势</strong>在于：查询只需读取涉及的列数据，从而显著减少磁盘扫描的数据量（尤其在宽表、只查询少数列时优势明显）。同时，同一列的值类型相同且分布相近，压缩效率更高。Doris 对列数据采用多种压缩编码以降低存储占用并提高读写效率。例如，整数类型可能使用 RLE、位图等编码，字符串可能使用字典编码等。Segment 文件每列的元信息 (<code>ColumnMeta</code>) 中记录了该列的数据类型 (<code>Type</code>)、长度 (<code>Length</code>)、编码方式 (<code>Encoding</code>)和压缩算法 (<code>Compression</code>) 等。常见压缩格式包括 LZ4、ZSTD 等无损压缩，用于平衡压缩率和解压速度。Doris 会根据列类型和数据分布选择合适的编码与压缩，如低基数字符串列适合字典编码+压缩，高基数整型可能直接比特压缩等。这些策略都封装在 Segment 的存储引擎中，对用户透明。</p>
<p>Segment 文件的结构遵循 <strong>Segment V2</strong> 格式，主要由数据页、索引和文件尾 (Footer) 组成。每个 Segment Footer 包含整个 Segment 的元数据汇总，包括总行数 (<code>num_rows</code>)、各列的 meta 信息和索引元信息等。Footer 通常很小，位于文件末端，读取 Segment 时先加载 Footer 以获取索引入口。对于每列，除了数据页序列外，还维护了若干辅助结构以支持快速定位和过滤。这些索引机制在下一节详细介绍。在数据写入流程中，BE 会将 MemTable 中的行格式数据转换为列格式，按照每列生成编码后的数据页，并在 flush 最后生成 Footer 写入磁盘。<strong>MemTable</strong> 使用 SkipList 数据结构维护插入顺序，实现按主键排序。Flush 分两个阶段：首先将行数据转为列存并为每列构建索引，其次将列数据页序列和索引写入 Segment 文件。通过列式存储结合压缩，Doris 在实际生产中往往能达到高达10倍以上的压缩比，同时获得更高的IO吞吐率和更低的CPU解压开销（具体压缩效果视数据而定）。这种列式文件格式是 Doris 提供高性能查询的基石之一。</p>
<h3 id="2-3-多级索引机制"><a href="#2-3-多级索引机制" class="headerlink" title="2.3 多级索引机制"></a>2.3 多级索引机制</h3><p>为了加速海量数据的查询过滤，Doris 在存储层设计了多级索引，包括前缀索引、Ordinal 索引、ZoneMap、Bitmap、Bloom Filter，以及新版的倒排索引等。索引按用途可分为<strong>点查索引</strong>和<strong>跳跃索引</strong>两大类：</p>
<ul>
<li><p><strong>点查索引</strong>：优化点查询，通过索引直接定位满足条件的行集。当 WHERE 条件命中行非常少时效果最佳。Doris 的点查索引包括：</p>
<ul>
<li><p>**前缀索引 (Short Key Index)**：针对主键/排序键建立的稀疏索引。Doris 将表定义的 Key 列作为排序列存储，并在每个 Segment 中每隔固定行数（默认1024行）抽取该行的前几个Key列值，形成一个索引项。每个索引项代表“该Segment中此后1024行的起始Key值”。查询涉及排序列的等值或范围条件时，Doris可利用前缀索引通过二分查找快速定位到目标Key的大致位置。前缀索引用于 Duplicate、Aggregate、Unique 等模型的主键列，默认取前最多3列且总长度不超过36字节作为索引前缀，遇到字符串也只截取前缀部分。这样在查询如 <code>WHERE key1 = X AND key2 = Y</code> 时，可直接跳转到接近 (X,Y) 开头的Segment偏移处扫描，而不必全文顺序扫描。需要注意前缀索引只是粗粒度定位（粒度为索引间隔1024行），精确查找仍需对落入范围的行做比较验证。但总体能显著缩小扫描范围，加速点查或小范围查询。</p>
</li>
<li><p><strong>倒排索引 (Inverted Index)<strong>：从 Doris 2.0 开始支持，用于对指定列建立 <strong>值 -&gt; 行号列表</strong> 的映射。倒排索引特别适用于全文检索、模糊匹配和非键列等值/范围查询，可将扫描复杂度从按行遍历降为直接定位行集合。Doris 实现的倒排索引将每行视为一个文档，每个被索引列相当于文档的一个字段。对文本列，倒排索引存储每个关键词对应出现该词的行ID列表；对数值或日期列，存储每个具体值对应的行ID集合。查询时，例如 <code>WHERE text_col MATCH &#39;keyword&#39;</code>，会通过倒排表直接得到包含该关键词的行ID集合，再据此读取对应行数据，而无需扫描整列。对于范围查询，倒排索引可存储每个值或值区间对应行集，从而快速过滤范围内行。Doris 将倒排索引存储在独立文件，与数据文件一一对应但物理上独立。这种设计使得创建和删除索引无需重写数据文件，降低维护开销。值得一提的是，倒排索引功能</strong>已基本取代原先的 Bitmap 索引</strong>，因为倒排索引不仅能处理等值，也支持范围和全文匹配，应用更广。Doris 提供 SQL 语法 <code>CREATE INDEX ... USING INVERTED</code> 来创建倒排索引，2.0+版本中对 Aggregate/Unique 模型的Key列和Duplicate模型任意列均可建立倒排索引（某些模型有细微限制）。实测表明，引入倒排索引可使一些文本检索场景性能提升数十倍。例如对日志字段建倒排索引，可比LIKE模糊查询快40倍以上。</p>
</li>
</ul>
</li>
<li><p><strong>跳跃索引</strong>：也称<strong>跳数索引</strong>，用于跳过不满足条件的数据块，实现粗粒度过滤。其原理是利用索引标记每个数据块的概要信息（如值范围），查询时跳过明显不可能包含目标值的块，只读取可能有用的数据块再精确过滤。当查询结果行较多、需要扫描大量数据时，跳跃索引可减少扫描总量、改善性能。Doris 提供的跳跃索引包括：</p>
<ul>
<li><p><strong>Zone Map 索引</strong>：对每列数据的每个Segment和每个页维护<strong>最小值、最大值以及是否含 NULL</strong>。这些统计信息即所谓 Zone Map。Doris 在导入数据时自动为所有列构建 ZoneMap（Duplicate 模型下所有列都有ZoneMap，Aggregate/Unique 模型则对 Key 列构建）。查询包含 <code>col = value</code> 或范围 <code>col &gt; value</code> 等条件时，系统先检查各 Segment/Page 的最小最大值：如果 value 不在 [min, max] 区间内则整个数据块不可能满足条件，直接跳过读取。同理，若查询 <code>WHERE col IS NULL</code> 而某块标记无 NULL，则可跳过。通过 ZoneMap，很多无关块在扫描前即被排除，大幅减少I/O。ZoneMap 是一种轻量索引，存储开销很小，却对等值和范围过滤极为有效。在宽表多列场景下，ZoneMap 可结合多列条件发挥乘积过滤效应。例如 <code>WHERE A &gt; 5 AND B = &#39;X&#39;</code> 同时利用两列各自ZoneMap筛除不满足条件的块。</p>
</li>
<li><p><strong>Bloom Filter 索引</strong>：针对高基数列的加速索引。BloomFilter 是一种概率型结构，用一组哈希位判断某元素是否可能存在于集合。Doris 允许为字符串、数值等高基数字段创建 Bloom Filter 索引（建表或 ALTER 时指定）。Doris 的 BloomFilter 索引在存储实现上以<strong>Data Page为粒度</strong>：每个数据页对应一个 BloomFilter，表示该页可能包含哪些值。查询如 <code>WHERE email = &#39;abc@xyz.com&#39;</code> 时，会先加载该列的 BloomFilter 索引项，只读取那些 BloomFilter 判断“可能包含此值”的页，反之确定不可能包含的直接跳过。BloomFilter 可能有少量误判（返回少量空扫描页），但可接受；确定性地过滤大量不相关页则显著提升性能。BloomFilter 尤其适用于类似用户ID、UUID 等独特值的等值查询。相比 Bitmap（需要存储每个值所有行号），BloomFilter 存储开销固定且随行数线性增长，适合高基数。Doris 创建 BloomFilter 索引非常灵活，用户按需为特定列开启即可，每个数据页索引项在 flush 数据时生成并写入 Segment 文件。在查询执行时，BE 从 Segment Footer 定位 BloomFilter 索引区域，加载相应页的过滤器进行判断。综合来看，BloomFilter 对<strong>点查大表</strong>是非常有效的“跳跃索引”，可在大幅减少扫描的同时保持较低维护开销。</p>
</li>
</ul>
</li>
</ul>
<p>此外，Doris 早期还支持 <strong>Bitmap 索引</strong> 作为手动创建的索引类型，主要针对低基数列（如性别、地区等枚举值）优化等值和 IN 查询。Bitmap 索引为每个不同值维护一个有序<strong>行号集合</strong>（Roaring Bitmap格式），并存储一个全局<strong>值字典</strong>。查询时通过查字典定位值对应的位图，再直接获取行号列表以定位数据。Bitmap 索引在 Doris 中需要用户显式 <code>CREATE INDEX</code> 创建，适用于 Duplicate/Primary 模型任何列，以及聚合/更新模型的维度列。由于 Bitmap 索引不能直接加速范围查询，且高基数列位图会很大，Doris 引入倒排索引后逐步以倒排取代 Bitmap 功能。然而在某些典型低基数枚举场景（如性别、布尔标记），Bitmap 仍然是一种空间高效且查询快速的索引方案。未来 Doris 社区可能继续保留 Bitmap 作为特殊索引，但更推荐使用功能更通用的倒排索引。</p>
<p>值得一提的是，Doris 在 Segment 内还使用了<strong>Ordinal 索引</strong>这种内部一级索引结构来加速列存取。Ordinal Index 为每列的每个 Data Page 记录该页在文件中的偏移、大小和起始行号。所有页索引项集中存储在 Ordinal Index Page，Segment Footer 中保存该索引页的偏移和长度。查询时 Doris 会<strong>先加载 Ordinal Index</strong>，根据所需的行号范围快速定位对应数据页的文件位置，再跳转读取，从而避免逐页线性扫描。Ordinal Index 将列存储的数据按行号对齐，为其他索引（前缀索引、倒排索引等）最终定位具体数据行提供了关键桥梁。实际上，Ordinal Index 可看作 Segment 内部的“一级索引”，而前缀/倒排/ZoneMap/BF 等属于更高层次索引，需要通过 Ordinal Index 间接定位物理数据页。由于 Ordinal Index 是存储引擎自动维护的，并且对于多页列才生成，因此对用户透明，但其作用非常重要：保证了列式存储也能高效地随机定位到指定行所在的数据块，进而配合其他索引实现快速精准的数据访问。</p>
<h3 id="2-4-数据写入与版本管理"><a href="#2-4-数据写入与版本管理" class="headerlink" title="2.4 数据写入与版本管理"></a>2.4 数据写入与版本管理</h3><p>在 Doris 中，数据写入采用**微批（Mini-batch）**的方式批量加载，每批次数据为每个 Tablet 形成一个新的 Rowset。当用户通过 Stream Load、Insert 或 Broker Load 导入数据时，FE 会将数据分发到对应分片 Tablet 后，BE 侧在内存中为每个 Tablet 创建一个 <strong>DeltaWriter</strong>，对应一个 MemTable。DeltaWriter 持续接收这一批的数据并写入 MemTable（底层使用 SkipList 保证插入顺序），当 MemTable 达到一定大小或导入完成后触发 flush。Flush 时，MemTable 中的行按主键已排序，BE 将其转为列格式并生成索引，然后写出 Segment 文件。Flush 完成即在该 Tablet 下生成一个新的 Rowset，版本号通常为自增的当前最大版本号。FE 在所有涉及 Tablet 都 flush 成功后，会发布事务使新版本对查询可见，并将 Rowset 元数据加入 FE 的元数据管理中。在 Unique Key 表中，如果开启了 Merge-on-Write 特性，新版本会自动合并并覆盖相同主键的旧记录（具体实现见下节），而在 Duplicate Key 表中则直接追加多版本。</p>
<p>Doris 通过版本号 (<code>version</code>) 机制实现<strong>多版本并发控制</strong>。每个 Rowset 带有 [开始版本, 结束版本] 区间，通常区间两端相同表示一个增量版本。当进行 <strong>Delete 删除</strong>操作时，Doris 不直接物理删行，而是生成一个特殊的删除标记 Rowset，记录被删主键或范围。这个 Delete Rowset 与普通数据 Rowset一样参与版本链。查询时，如果某行的最大版本来自 Delete Rowset，则该行会在结果中过滤掉，实现逻辑删除。这种设计保证删除可快速生效且不阻塞查询，真正物理删除延后通过 Compaction 完成。<strong>Compaction</strong> 作为后台任务，由 BE 定期触发，将一个 Tablet 下多个相邻版本的小 Rowset 合并为一个大 Rowset，从而减少文件数和加速读取。Compaction 包括 <strong>Base Compaction</strong>（合并很多小版本）和 <strong>Cumulative Compaction</strong>（合并最近的增量版本），策略上通常先累计合并近期小文件，周期性触发全面合并。合并时需要读取参与的 Rowset，合并排序并应用删除标记，然后输出新 Rowset 替换老版本范围。Compaction 能够控制数据文件大小、淘汰过时版本并应用所有删除，保证存储健康和查询性能。</p>
<h4 id="主键模型下的数据更新"><a href="#主键模型下的数据更新" class="headerlink" title="主键模型下的数据更新"></a>主键模型下的数据更新</h4><p>值得关注的是，Doris 支持<strong>Unique Key 主键模型</strong>来实现对数据的更新覆盖。这在 1.2 版本后通过 <strong>Merge-on-Write (MoW)</strong> 实现了高性能实时更新。Unique Key 表定义一个主键列组合，导入新数据时如果主键已存在则替换旧值而非累加。Doris 提供两种主键存储模式：<strong>Merge-on-Read (MoR)</strong> 和 <strong>Merge-on-Write (MoW)<strong>。MoR 模式下，数据写入仍按增量 Rowset 存储，不合并冲突，由查询在读取时对同主键多版本进行聚合去重（类似 HBase 的查询合并思路）。MoW 模式则在写入阶段就将同主键的旧版本标记删除、新数据写入新版本，这需要维护主键索引来快速定位旧记录。Doris 在 2.0 起支持 MoW 并在 2.1+ 默认 Unique Key 表使用 MoW 存储。MoW 通过在 BE 存储中为 Unique 表建立一个 <strong>Primary Index</strong>（主键索引），可理解为内存跳表或树结构记录每个主键当前最新版本的位置。导入时如果遇到相同主键，系统会将旧值的版本做删除标记，并插入新值。Compaction 时也会利用该索引只保留最新记录进入新文件，从而实现</strong>以空间换时间</strong>的即时更新能力。Unique Key MoW 大幅提升了查询效率（无需每次读合并），使 Doris 能支撑高频实时更新场景，如业务指标的滚动更新等。测试显示开启 MoW 后，Unique 表查询性能可提高1倍以上，代价是写入需要更多内存维护索引且数据存储量增加（因多存储一份删除标记等）。Doris 2.1 报告显示 Unique 表 MoW 的存储开销约是 Duplicate 表的2倍，但换来了查询的直接访问效率。为优化存储，Doris 3.0 支持了<strong>按列选择行存储</strong>以减少行复制数据量：用户可指定部分列参与 RowStore，从而仅缓存关键列以支持点查，而不复制整行。总之，借助主键模型和存储引擎改进，Doris 可在实时数仓场景中实现批处理与<strong>近实时更新</strong>的融合，兼顾了读性能和更新及时性。</p>
<p>通过以上机制，Doris 的存储层实现了对 OLAP 负载的优化：列式组织、压缩编码提升了扫描效率，多级索引减少了不必要的数据读取，而分片和多版本机制保证了并发与更新需求。存储引擎的设计充分考虑了“<strong>批量导入，少量更新；绝大多数读；宽表扫描少数列</strong>”的典型分析场景特点。实际案例表明，在正确建模和索引配置下，Doris 可以在单表十亿行级别上仍提供秒级甚至亚秒级查询响应，存储层功不可没。</p>
<h2 id="3-查询执行引擎"><a href="#3-查询执行引擎" class="headerlink" title="3. 查询执行引擎"></a>3. 查询执行引擎</h2><p>Apache Doris 构建了高效的分布式查询执行引擎，依托 MPP 架构、向量化处理和 Pipeline 并行技术，实现了对复杂 SQL 的高性能执行。本节详细介绍 Doris 查询的执行流程、优化技术、以及关键特性如谓词下推和向量化原理。</p>
<h3 id="3-1-查询执行流程"><a href="#3-1-查询执行流程" class="headerlink" title="3.1 查询执行流程"></a>3.1 查询执行流程</h3><p><strong>SQL 查询流程</strong>从客户端发送查询请求（使用 MySQL 协议）开始。FE 接收到 SQL 后，首先进行<strong>解析与优化</strong>：包括词法/语法解析生成抽象语法树，语义校验（检查表列存在、类型兼容等），然后生成单节点的逻辑执行计划。接着，FE 基于元数据和统计信息，将逻辑计划转换为分布式物理计划（这一阶段 Doris 使用规则优化为主）。典型的优化包括：谓词下推到扫描节点、列剪裁（只读取需要的列）、选择合适的 Join 策略（广播小表或shuffle大表）等。Doris 引入了<strong>Cost-Based Optimizer (CBO)</strong> 的部分功能，在面对 Star Schema 等查询时能够基于代价选择较优的执行路径。不过总体而言，Doris 的优化更偏向规则简化，例如按照固定策略进行Join重排，而StarRocks等衍生版本在CBO上投入更多。优化完成后，FE 把物理计划划分为多个<strong>Plan Fragment</strong>（计划片段）。每个 Fragment 通常对应查询计划中自成一体的一段算子链，比如一个或一组 join 后的聚合。Fragment 之间通过 Data Stream Buffer（Exchange 算子）连接，表示需要跨节点传输数据。FE 会为每个 Fragment 决定<strong>并行度和执行节点</strong>：如Scan Fragment 并行度等于涉及的 Tablet 副本数，Join Fragment 并行度取决于数据分区方式等。然后 FE 将 Fragment 和参数（如筛选条件、需要扫描的分片列表）发送给对应的 BE 节点执行。</p>
<p>在 BE 侧，每台节点收到一个或多个 Fragment 实例的执行请求，BE 为每个实例启动相应的<strong>执行线程/Pipeline</strong>。Scan 类 Fragment 直接在本节点访问存储引擎扫描数据；需要聚合或Join的 Fragment 则可能等待来自别的节点的数据流。各 BE 间通过 Doris 自身的高效 RPC 通道（基于 Brpc）进行数据交换：例如一个 shuffle join，两个 BE 会按照分区键将中间数据按哈希分发给彼此对应的 Fragment 实例。Doris 会在 BE 内部分配足够的<strong>线程</strong>处理并发的 fragment 任务，同时利用 I/O 与 CPU 并行隐藏延迟。在整个执行过程中，FE 作为调度者，会监控各 Fragment 执行状态，若某节点失败可重试将任务转移其他副本节点执行（利用Tablet多副本）。执行完成后，各BE将结果数据（或部分聚合结果）汇集回 FE 或直通返回客户端。对于简单查询（如单节点聚合），FE 可直接合并 BE 返回结果然后返回；对于大结果集，Doris 也支持 BE 直接将结果以流式方式发送给客户端（通过 FE 转发）。整个流程类似经典的 MPP 数据库，如 Greenplum 或 Spark SQL，但 Doris 因为 FE/BE高度一体，减少了层间开销，性能更佳。据官方介绍，Doris 对TPC-H、TPC-DS查询的平均响应时间可比Presto/Trino快3～5倍。这是得益于其紧凑的执行流程和向量化引擎。</p>
<h3 id="3-2-MPP-并行与数据分发"><a href="#3-2-MPP-并行与数据分发" class="headerlink" title="3.2 MPP 并行与数据分发"></a>3.2 MPP 并行与数据分发</h3><p>Doris 的查询执行遵循 MPP (Massively Parallel Processing) 模式，即<strong>全分布式并行执行</strong>。一条查询在Doris内部被拆解成多个可并行的任务，由多个BE协同完成。并行执行体现在两个层面：<strong>数据并行</strong>和<strong>任务并行</strong>。数据并行指利用多节点分别处理不同数据分片，例如Scan阶段每个BE扫描不同Tablet的数据，实现分区并行。任务并行指不同算子流水线可以在不同线程上并发执行，例如一个节点上同时进行join处理和聚合处理。Doris 会充分利用集群的多核多机资源，将大型查询拆成许多子任务并发运行，从而随节点数扩展提供接近线性的性能提升。</p>
<p><strong>数据分发与Shuffle</strong>是MPP执行的核心环节。在一些阶段需要将数据按照键重新分配，比如在分布式 Join 中，双方表的数据需要按 Join Key 分区后再合并匹配。Doris 通过 <strong>Exchange 算子</strong>实现数据传输和分发。在执行计划中，Exchange 算子标记了一个分界点，表示需要网络传输的数据流。FE 在生成物理计划时，会根据需要插入 Broadcast Exchange（广播小表到每个节点）或 Shuffle Exchange（按照哈希将数据 repartition）等策略。运行时，每个BE上的Exchange Source算子会将本节点数据打包，通过网络发送给目标BE上的Exchange Sink算子。为了避免网络拥堵和提升吞吐，Doris 的发送通常采取<strong>分片并行</strong>发送：即每个节点并非统一使用单连接发数据，而是将数据划分成多个流进行并行传输（类似于Spark的MapOutput分块思想）。Doris 3.0 在调度上引入了任务分片和权重机制，更好地均衡节点间的数据流量。当文件/分区极多时，还采用异步拉取分片的方式降低FE内存压力。这些改进确保了在大规模 Shuffle 时，网络利用率高且不会因为单节点瓶颈导致整体拖慢。</p>
<p><strong>Join 策略</strong>方面，Doris 支持广播和分布式 Shuffle 两种。对于较小的维度表，FE会选择 Broadcast Join：将小表数据广播到每个参与Join的大表分区节点，在本地完成Join。这避免了对大表数据洗牌，往往更高效。对于两个大表或无法广播的情况，则使用 Shuffle Join：按Join键对双方分区并在对应节点匹配。Doris 会根据表统计和经验规则选择策略。StarRocks 等衍生产品进一步实现了基于代价的动态切换，但 Doris 在主要场景下的规则选择也能取得较好性能（例如自动广播一定大小以下的表）。此外 Doris 还支持 <strong>Colocation Join</strong> 优化：如果两张表按同样的分片键和分片数量存储，那么它们的对应Tablet分区天然在每个BE上对齐，此时FE会选择<strong>本地Join</strong>而无需Shuffle。这要求建表时指定 colocate 属性且分区键匹配。Colocation Join 可显著提升多表星型模型查询性能，因为完全避免了数据重分发成本。</p>
<p><strong>Pipeline Pipeline并行执行</strong>（见3.4节）在 Doris 2.0 引入后，更加充分地发挥了 MPP 的并行潜力。传统volcano模型下，一个Fragment内算子逐个处理批次数据，存在上游阻塞下游的问题。而 Pipeline 模式将算子拆分成细粒度管道，允许数据边生成边消费，大幅提高流水线并行度和CPU利用率。Doris 2.0 默认开启 Pipeline 引擎，并在后续版本不断优化，如自适应调整Pipeline并发度等。总之，Doris 的 MPP 执行框架使其能够高效利用大型集群资源，对复杂 SQL 进行全并行处理。这也是其相较单机数据库或只并行扫描的引擎（如Spark/Presto）的重要性能优势。官方测试显示，Doris 在并发大查询场景下表现出色，TPC-H 100GB规模下性能超过Spark SQL数倍，逼近C++原生引擎ClickHouse的水平，同时保持了良好的SQL兼容性和易用性。</p>
<h3 id="3-3-谓词下推与分区裁剪"><a href="#3-3-谓词下推与分区裁剪" class="headerlink" title="3.3 谓词下推与分区裁剪"></a>3.3 谓词下推与分区裁剪</h3><p><strong>谓词下推 (Predicate Pushdown)</strong> 是关系查询优化的重要手段，在 Doris 中得到充分运用。谓词下推指将 SQL 查询中的过滤条件尽可能提前应用在数据扫描阶段，从而减少后续处理的数据量。Doris 在解析查询计划时，会将 <code>WHERE</code> 子句中的过滤条件下推到 ScanNode，使存储引擎在读文件时就进行条件判断，跳过不符合条件的数据块或行。这与 Doris 存储层的各种索引结合，极大提高了过滤效率。例如，对于 <code>WHERE col1 = 100 AND col2 &gt; 5</code>，FE 会将这些条件附加到扫描计划中，BE 执行扫描时一边读数据一边用 ZoneMap/BloomFilter/前缀索引等筛选，只产出匹配条件的行进入算子流水线。尤其是针对分区字段的谓词（如 <code>date = &#39;2023-07-01&#39;</code>）和分桶键的谓词，Doris 能在 FE 阶段直接完成<strong>分区裁剪</strong>和<strong>分片裁剪</strong>：FE 会基于元数据判断哪些分区、哪些 Tablet 包含满足日期=2023-07-01的数据，只为相关 Tablet 创建扫描任务，其余 Tablet 根本不参与本次查询。这种 Partition Pruning 可以显著减少大表扫描范围——例如表按日期分区，查询单日数据仅扫描1/N的数据。再如哈希分桶键作为 join key 时，Doris 也能在某些等值查询下将过滤推至特定桶。分区裁剪和Bucket裁剪由 FE 利用元数据完成，对用户透明。</p>
<p>在外部表查询中，Doris 同样会尽可能执行谓词下推。例如查询 Hive 外部表，Doris 通过 Hive Metastore 获取该表分区列信息，能将相应过滤条件转化为 Hive 分区过滤，从源头减少数据读取。对于 Iceberg/Hudi 等数据湖表，也可利用其元数据（如索引或Manifest）进行文件级裁剪。这些优化使 Doris 作为查询加速引擎时，能高效地“<strong>以需索取</strong>”，大幅降低跨系统查询的代价。</p>
<p><strong>向量化谓词计算</strong>也是 Doris 性能优势之一。Doris 的存储引擎将列数据读出后，会以批为单位在 CPU 上进行向量化的筛选计算。比如对一个批1024个值应用 <code>&gt; 5</code> 条件，内部使用SIMD指令同时计算多个值的比较，再将结果作为位掩码滤除不符合的行。这比逐行判断要快得多，特别在Intel AVX2/AVX512等指令集支持下，可以8或16个值并行比较。对于字符串like模式匹配等复杂谓词，Doris 也做了针对性优化（如倒排索引预筛选）。另外，Doris 利用<strong>Runtime Filter</strong>进一步提升Join场景的谓词下推：在分布式Hash Join中，构建端BE会生成一个包含键值的BloomFilter，通过FE下发给探测端BE，探测端在扫描其表时即可用此Filter排除不可能匹配Join键的记录，减少数据发送量。这种<strong>动态分区剪切</strong>在Doris 2.x中得到支持，类似技术在Impala/Presto也验证有效。在 Doris 实践中，Runtime Filter 常使一些关联查询网络传输数据量降低90%以上，整体性能提升2-5倍（特别在大表join大表场景）。</p>
<p>综合来说，Doris 将各种谓词尽早施加在存储读取阶段，<strong>“读所需的数据”</strong>而非<strong>“读后再筛选”</strong>。再配合列式存储和索引，绝大部分无关数据都被过滤在IO阶段，极大减轻计算负载。这也是 Doris 能实现子秒级查询的关键因素之一。正如一篇对 Doris 索引的深入分析所总结：“通过精确匹配行号，减少存储层需要扫描的数据量，从而显著优化了 WHERE 子句过滤操作”。</p>
<h3 id="3-4-向量化执行与-Pipeline-引擎"><a href="#3-4-向量化执行与-Pipeline-引擎" class="headerlink" title="3.4 向量化执行与 Pipeline 引擎"></a>3.4 向量化执行与 Pipeline 引擎</h3><p><strong>向量化执行</strong>是 Doris 查询引擎的一项核心优化。传统的关系数据库执行采用基于行的 Volcano 模型，每次处理一行数据，函数调用频繁，解释器开销大。而向量化执行以<strong>批（Block）</strong>为基本处理单元，每次算子处理成百上千行，减少函数调用和解释开销，并且有利于使用CPU SIMD指令提升算术运算效率。Doris 从早期版本（0.14+/0.15）就引入了向量化执行框架，算子之间传递 <code>Block&lt;Row&gt;</code> 而非单行，大部分算子逻辑都使用批量处理算法。尤其是在聚合、算术表达式计算等场景，向量化可显著提速。以一次简单求和为例，向量化可以让CPU一条指令同时对8个数求和，而逐行则需要8次指令循环。Doris 官方统计表明，开启向量化后查询吞吐可提升3~5倍不等，对计算密集型查询效果尤为明显。ClickHouse 作为业内知名的极致性能引擎，也是在早期就采用向量化执行引擎。Doris 的实现充分借鉴了这一理念，并结合Java+C++混合架构特点做了优化。例如 FE 阶段在Java里不会真实处理数据，只生成计划；BE 阶段C++对每个算子都提供了向量化接口，算子处理函数对Block进行迭代。Doris 1.1+还支持<strong>向量化表达式评估</strong>，将复杂表达式（CASE WHEN等）拆解为算子内的矢量运算，大幅降低了解释求值开销。</p>
<p><strong>Pipeline 执行引擎</strong>是向量化的进一步提升。Pipeline 模式将查询执行划分为多个细粒度的管道，每个管道是一系列算子的串联，多个管道可以并发执行且异步调度，从而更充分利用多核和避免单个算子阻塞全局。Doris 在 2.0 版本引入 Pipeline 引擎作为实验特性，2.0 之后默认启用 Pipeline 模式代替原有的Volcano模式。在 Pipeline 模式下，查询计划被切成更小的任务单元（Task），每个 Task 由若干算子构成且不产生阻塞。例如，将一个 join+agg 划分为 build hash table、probe side join、agg 三个Task。这样BE可以在join未完全结束时，就启动agg任务对已输出部分进行聚合。Pipeline 引擎通过<strong>异步数据驱动</strong>的方式，使上游算子一旦有数据就立刻推动下游算子处理（而非等待全局同步点）。此外 Pipeline 引擎允许同一算子开出多个并行实例提高吞吐。例如以往一个 Join 节点在一个BE进程只有一个线程，现在Pipeline可让它并行开N个线程处理不同数据块。Doris Pipeline 引擎还实现了<strong>自适应并行度</strong>和<strong>动态调度</strong>：根据运行时各Pipeline的消费速度，调整线程资源分配，避免下游慢导致上游堆积或资源浪费。这些技术能显著提升CPU利用率和减少查询尾延迟。</p>
<p>据官方博客介绍，采用 Pipeline 引擎后，Doris 在高并发复杂查询场景下性能有大幅提升，资源利用更均衡。Doris 2.0 将 Pipeline 引擎默认打开，同时改进内存释放机制防止Pipeline增加并发度后内存占用过高。测试表明，在一些JOIN+AGG查询中Pipeline可以减少30%以上的执行时间，在并发情况下效果更明显。StarRocks 数据库也是在2021年率先采用了类似Pipeline机制，并证明对长查询和高并发短查询都有优化。Doris 社区跟进这一方向，使Apache Doris逐步具备与新一代OLAP引擎相当的执行效率。</p>
<p>总结而言，<strong>向量化 + Pipeline</strong>的组合，使 Doris 的查询执行模式更加贴近现代CPU的最佳实践：批量处理降低解释开销，流水线并行充分利用多核和隐藏I/O等待。这套引擎优化让 Doris 在实时分析场景下既能对单个复杂查询发挥最大性能，又能承载高并发的混合查询而保持稳定的吞吐。对比之下，早期的Greenplum等主要采用进程并行但算子未必向量化，Presto采用多线程Pipeline但缺少存储引擎优化。Doris 则将存储、执行、网络各层面结合优化，真正做到软硬件高效协同，实现了<strong>行业领先的查询速度</strong>。</p>
<h3 id="3-5-高并发与低延迟优化"><a href="#3-5-高并发与低延迟优化" class="headerlink" title="3.5 高并发与低延迟优化"></a>3.5 高并发与低延迟优化</h3><p>除了上述面向单查询性能的优化，Doris 还针对高并发短查询（如即席查询、Dashboard刷新等）做了专门优化，确保在<strong>高QPS场景下保持低延迟</strong>。首先，Doris 支持<strong>短查询路径 (Short Path)<strong>优化。当检测到查询是简单的点查询（如主键等值查1行），FE 会规划特殊的简化执行路径，绕过一些开销较大的步骤。例如对于 <code>SELECT * FROM tbl WHERE id = 123</code> 这种点查，FE 不做复杂优化，BE 也不走常规迭代器流程，而是调用专门的API直接通过主键索引定位行返回。这减少了SQL解析和计划开销，使得单次点查延迟可以降低到毫秒级。要启用短路路径，需要在Unique Key表上打开 <code>enable_unique_key_merge_on_write</code> 和 <code>store_row_column</code> 属性（即开启主键行存储），这样主键查找只需一次RPC即可完成。Doris 还提供</strong>行存加速</strong>模式：上文提到开启 <code>store_row_column</code> 会为Unique表额外存一列行式数据，以牺牲存储换取点查不必逐列汇总。结合MoW主键索引，Doris 实现了<strong>键值查询一跳直达</strong>（Short-Circuit），满足在线服务亚毫秒级查找需求。需要注意短路径仅适用于简单单表主键等值查询，不支持复杂查询。启用后在Explain计划中会显示 <code>SHORT-CIRCUIT</code> 标志。</p>
<p>其次，在FE端，Doris 针对大量重复查询提供了<strong>Preprare语句缓存</strong>和<strong>Query结果缓存</strong>。FE 的 SQL 解析和计划对CPU有一定开销，高并发下FE可能成为瓶颈。Doris 支持 MySQL 协议的 PreparedStatement 功能，当使用准备语句执行时，FE 会将解析和表达式绑定结果缓存，后续相同语句不同参数可跳过解析直接使用缓存的执行计划。测试显示，对简单点查使用 Prepare 可将FE端CPU开销降低75%，使整体延迟降低4倍以上。Doris 1.2+ 默认开启服务器端 Prepare 支持（需JDBC参数 <code>useServerPrepStmts=true</code>），极大优化了高QPS情况下FE的处理能力。</p>
<p><strong>查询结果缓存</strong>则是从另一个角度减少重复计算。Doris 提供两种粒度的结果缓存：<strong>SQL Cache</strong> 和 <strong>分区 Cache</strong>。SQL Cache 记录完全相同的查询SQL及其结果，后续若有字符串完全一致的查询请求直接返回缓存结果，避免执行。Partition Cache 更进一步支持不同查询共享缓存——只要查询的数据分区范围相同，它可以命中缓存。例如Dashboard上7天趋势图，多次刷新SQL可能只是时间范围不同但包含相同历史区间数据，Partition Cache 能缓存每个分区段的结果并拼接，命中率更高。Doris 通过对缓存数据加上版本号来保证<strong>缓存与源数据一致</strong>，一旦底层表更新导致分区版本变更，对应缓存项即失效。缓存数据存储在 BE 内存中，采用一致性哈希保证节点上下线缓存平稳迁移，并用改进的LRU算法管理淘汰。监控指标表明，在多报表重复查询场景下启用结果缓存可显著降低99%分位延迟和BE CPU负载。值得一提，Doris 提供了细粒度的缓存控制参数，如最大缓存行数3000行、最大数据量30MB（默认）等，以避免缓存巨量结果浪费内存。用户可根据业务需要调整这些参数。</p>
<p>再次，Doris 设计了<strong>Page Cache 与行缓存</strong>机制优化IO与内存访问。BE 内部对已读取的列数据页有一层缓存（LRU缓存），再次访问相同数据页可直接从内存取。由于列式页通常较大，这种缓存主要对重复扫描或热点数据有效。但当开启行存储（store_row_column）后，每行包含多个列拼成的“行块”，此时列式页缓存可能效率不高，因为行访问模式跟列访问不一样。为此 Doris 引入<strong>行缓存</strong>（Row Cache）：一个专门缓存行格式数据块的LRU缓存，占用BE内存的默认20%。行缓存提高了点查或只取少量列场景下的缓存命中率。比如一个用户详情查询频繁按uid查整行，如果该uid行块在Row Cache中，将极快返回。用户可通过BE配置参数 <code>disable_storage_row_cache=false</code> 开启行缓存，以及调整 <code>row_cache_mem_limit</code> 控制其内存占比。在高并发小查询下，行缓存的加入明显降低了平均磁盘IO请求，进而降低查询延迟。</p>
<p>最后，在并发调度层面，Doris 支持通过<strong>多FE负载均衡</strong>和<strong>Observer读扩散</strong>来提升并发承载。可以将应用的查询请求通过ProxySQL、VIP等手段分散到多个FE实例。这样当单个FE CPU成为瓶颈时，可以线性增加FE节点来扩展。尤其是 Doris 的 Observer 角色FE正是设计用于<strong>读伸缩</strong>，它不参与写事务，仅处理查询且不阻塞主FE。在高并发场景下，建议部署1个 Master + 2~N 个 Observer FE，让查询主要打在 Observer 上，保持Master清闲以处理元数据变更等。这种架构被实践证明能有效避免Master在峰值时响应变慢。</p>
<p>通过以上多管齐下的优化，Doris 在既追求单查询低延迟又要求高吞吐的场景中表现出色。在一些互联网公司内部测试中，相比未优化的方案，Doris 在百万QPS级别的点查询场景下平均延迟降低到原来的1/5，并保持极高的并发伸缩性。这使得 Doris 除了作为分析型数仓外，也能初步满足部分在线查询、KV查询的需求，真正实现**“一站式”**的数据服务能力。</p>
<h2 id="4-数据导入与导出机制"><a href="#4-数据导入与导出机制" class="headerlink" title="4. 数据导入与导出机制"></a>4. 数据导入与导出机制</h2><p>Apache Doris 提供<strong>丰富且高效的数据导入与导出机制</strong>，支持批量导入离线数据和实时摄取流式数据，并可将查询结果或表数据导出到外部存储。根据场景不同，Doris 的数据接入方式可分为：实时写入、流式同步、批量导入和外部查询/导入四类。本节将介绍 Doris 的主要数据导入手段（如 Stream Load、Broker Load、Routine Load、DataX、Flink-Connector 等）和数据导出功能。</p>
<h3 id="4-1-实时数据写入"><a href="#4-1-实时数据写入" class="headerlink" title="4.1 实时数据写入"></a>4.1 实时数据写入</h3><p><strong>实时写入</strong>是指以较高频率将数据持续写入 Doris，要求秒级可见，用于实时分析。Doris 提供以下方式：</p>
<ul>
<li><p><strong>JDBC Insert</strong>：通过 MySQL 协议使用INSERT语句写入。适合每次数据量较小、频率较低（如每5分钟一次）的写入。Doris 支持INSERT … VALUES插入单行或多行，也支持INSERT … SELECT从其他表插入。INSERT操作以事务方式写入 Doris 表，默认每条 INSERT 作为一个事务提交。对于写入频率较高的情况，Doris 2.0 引入了<strong>Group Commit</strong>机制，可以在 FE 端将短时间内的多个小 INSERT 合并为一个事务以减少开销。用户需在 FE 配置开启 <code>enable_group_commit</code>，然后可以多并发使用INSERT，Doris 会批量处理。对于并发20以上或每分钟多次写入的场景，建议开启 Group Commit 或改用以下专门接口。</p>
</li>
<li><p><strong>Stream Load</strong>：Doris 原生提供的<strong>HTTP 协议数据加载接口</strong>，非常适合实时流数据导入。使用时，用户对 FE 提交HTTP POST请求（URL带 <code>_stream_load</code>），携带待导入的文件或数据流。Stream Load 支持 CSV、JSON、Parquet、ORC 等多种文件格式。每次 Stream Load 会在FE创建一个导入事务，将数据流经BE写入表并提交。它是<strong>同步</strong>导入：请求在数据导入完成并提交后返回结果。因此适用于中小批次、高频的数据，比如每分钟触发一次，将最新一批事件数据加载进 Doris。根据官方经验，单次Stream Load控制在百MB以下，可获得秒级导入延迟；如果数据量更大可并行多次Stream Load 或选择 Broker Load。很多上层数据同步工具（如 Flink Doris Connector、DataX Doris Writer）都是调用 Doris Stream Load 接口实现数据写入。实际上，Stream Load 是 Doris 最常用的导入方式之一，因为其使用简单（HTTP接口无需依赖复杂客户端）且能提供相对高吞吐。在最近版本中，Doris 也在优化 Stream Load 的性能和错误反馈机制。</p>
</li>
<li><p><strong>Routine Load</strong>：Doris 针对消息队列的<strong>持续导入</strong>方案。Routine Load 可以从 Kafka 等消息系统连续消费数据并写入 Doris 表。用户通过 <code>CREATE ROUTINE LOAD</code> 定义 Kafka连接信息、topic、分区以及数据格式（CSV/JSON），FE 会创建一个 Routine Load Job 并由调度线程持续运行。FE 将 Routine Load 任务切分为多个子任务并分发给BE消费不同Kafka分区的数据。Routine Load 的特点是<strong>Exactly-Once</strong>：使用两阶段提交协议，确保每条消息严格导入一次，不丢不重。它对接Kafka时可记录消费位点，支持自动断点续传，保障稳定性。Kafka Routine Load 默认支持 CSV和JSON格式消息解析。对于更复杂的消息格式（如 Avro、Protobuf），Doris 提供<strong>Doris Kafka Connector</strong>作为补充，下面介绍。Routine Load 适合需要<strong>低延迟持续摄取</strong>的场景，比如将业务数据库的Binlog经Kafka流式导入 Doris 实时数仓。实践中 Routine Load 可做到秒级延迟并支持高并发 partition 消费。不过需要注意流数据峰值过高时，可通过扩展 BE 数量或增加 Routine Load 并发度来处理。Routine Load 作业可以通过 SQL 暂停、恢复或停止，方便运维管理。总之，对于Kafka对接，Routine Load 是 Doris 提供的内建利器，具备Exactly-Once和自动容错能力，是流数据接入 Doris 的首选方案之一。</p>
</li>
<li><p><strong>Flink Doris Connector</strong>：这是在 Apache Flink 流处理框架中使用的 Doris 连接器，支持<strong>实时将 Flink 处理结果写入 Doris</strong>。该 Connector 由 Doris 社区提供。它底层利用 Doris Stream Load 实现 Exactly-Once 语义：Flink将每个微批结果作为一次 Stream Load 提交 Doris，并结合两阶段提交保证事务性。Flink Doris Connector 提供 DataStream API 和 Table API 两种方式，可以像使用其他 sink 一样方便地将流数据 sink 到 Doris 表。其支持INSERT、UPDATE、DELETE等操作（Unique Key模型下）以及读取 Doris 表作为 Flink 数据源。对于企业已有 Flink 流处理任务，希望把结果推到 Doris 做指标分析或实时数据服务，使用官方 Connector 是最省心的选择。配置上只需指定 Doris FE 地址、数据库、表、账户等，即可运行。Flink Doris Connector 在 Dora版本上支持 Flink 1.11~1.20 版本。它已经广泛应用于实时ETL和数据同步场景。例如通过 Flink CDC 捕获MySQL变更，经处理后写入 Doris 实时明细表。相比 Routine Load 直接从Kafka拉数据，Flink方案可以在中间进行灵活的数据清洗聚合，然后用 Connector 持续写入 Doris，提供了更大的实时计算能力。</p>
</li>
<li><p><strong>DataX Doris Writer</strong>：DataX 是阿里巴巴开源的离线数据同步工具，Doris 官方提供了 DataX Writer 插件，用于通过 DataX 任务将数据批量导入 Doris。DataX DorisWriter 底层也是调用 Stream Load 实现，将 DataX 从源头读出的批次数据写入 Doris。DataX 适合作为离线批处理调度（比如每天定时同步HDFS文件或关系库数据到 Doris），但其实时性不如 Flink。Doris 将 DataX 划归在<strong>流式同步</strong>类方式，因为 DataX 除批处理外也可配置监听 binlog 实时同步（类似Flink CDC）。无论如何，DataX 给不熟悉编程的用户提供了一个<strong>配置化工具</strong>来接入 Doris。许多企业用 DataX + Doris 实现了传统数据仓库迁移：比如用 DataX将Oracle、MySQL的数据按天全量抽取增量更新到 Doris，满足T+1报表或小规模实时同步需求。</p>
</li>
</ul>
<p>以上方式共同保证 Doris 在各种实时场景下的数据摄取能力。从<strong>每秒几次的小批量</strong>到<strong>每秒数百条的流数据</strong>，Doris 都有对应方案应对，并能确保数据一致性和低延迟。如果需求更复杂，比如<strong>主数据库CDC实时同步</strong>，可以组合Flink CDC捕获+Flink Doris Connector写入来实现，在保证事务一致前提下，达到<strong>准实时</strong>。总的来说，Doris 通过 Stream Load 提供了统一的高效导入接口，而 Routine Load、Flink Connector 等则各自封装场景细节，开发者可按需选用。</p>
<h3 id="4-2-批量导入离线数据"><a href="#4-2-批量导入离线数据" class="headerlink" title="4.2 批量导入离线数据"></a>4.2 批量导入离线数据</h3><p><strong>批量导入</strong>用于一次性加载大规模的离线数据，如每日增量文件、数据湖分区，或初始历史数据迁移等。Doris 支持多种批量导入方式：</p>
<ul>
<li><p><strong>Broker Load</strong>：Doris 经典的分布式批量导入机制，适合从 HDFS、S3 等外部存储读取大文件至 Doris。Broker Load 需要部署 Broker 进程（可与BE共部署），它充当 Doris 与外部存储间的代理。使用时，通过 <code>LOAD</code> 命令指定源文件路径（HDFS路径、S3 URL等）和文件格式，FE 会协调 Broker 从源拉取文件并将数据切分发送各BE写入。Broker Load 是<strong>异步</strong>导入：提交命令后立即返回，后台开始加载，用户可通过SHOW LOAD查看作业状态。它支持CSV/JSON/Parquet/ORC等格式文件的读取和按需转换。由于不需要先将文件下载到本地，Broker Load 能直接高效读取对象存储中的数据，在数据湖/仓库到Doris的对接中非常实用。Doris 现已逐步用<strong>HDFS Load</strong>替代Broker Load，即无需独立Broker进程而BE直接访问外部存储。2.0起 Doris BE 本身具有S3/HDFS访问能力，Broker进程非必需。无论具体实现，Broker/HDFS Load 能将上百GB到TB级别的数据并行导入 Doris 表，一般按分区并发导入，速度取决于外部存储IO。导入过程遇到格式错误可按配置容忍一定比例（max_filter_ratio），并在完成后提供错误文件下载。总之，Broker Load 是 Doris 面向<strong>大规模离线文件</strong>的主要手段，使用户无需先将数据搬到本地，直接实现云上存储到Doris的高速传输。</p>
</li>
<li><p><strong>Insert INTO SELECT</strong>：Doris 支持通过 SQL 将来自<strong>外部表或其他表</strong>的数据直接插入本地表。这种方式可用于异构数据源批量导入 Doris。例如，用户可以创建一个Hive外部表映射HDFS上的Parquet文件，然后执行 <code>INSERT INTO doris_table SELECT * FROM hive_table</code>，Doris 会读取Hive表数据并插入本地表。这实际上触发 Doris 内部的一个MapReduce作业：Scan外部数据 -&gt; 分发到各BE -&gt; 写入目标表Segment。Insert Select 的好处是用户以统一SQL完成，不需要额外工具；劣势是当前为<strong>同步执行</strong>，等待时间较长。不过 Doris 2.0 起支持将 Insert Select 封装进 <code>CREATE JOB</code> 进行<strong>异步执行</strong>。因此对超大型数据（几十亿行以上），仍建议用 Broker Load；而中等规模数据用 Insert Select 更方便。Insert Select 尤其适合<strong>湖仓一体</strong>场景：配合 Doris 外部表功能，可以无缝地把Hive/ICEBERG/HUDI的数据迁移进 Doris 或做增量同步。例如每天将Hive上一日增量 Insert Select 进 Doris 实时仓库。值得注意的是 Insert Select 也可用于Doris内部表间ETL，如汇总明细表到聚合表。执行上它也利用 Doris MPP引擎并行化，因此性能优于从外部手动抽取再导入。</p>
</li>
<li><p><strong>Spark Doris Connector</strong>：类似Flink，Doris 也有 Spark 的读写连接器，用于通过Spark加载数据到 Doris 或读Doris数据。Spark Doris Connector 可以将DataFrame直接保存(save)到 Doris表，会将DataFrame分区数据以并发任务形式通过Stream Load写入 Doris。这样可以利用Spark的分布式计算和数据准备，然后高速载入 Doris。在批量数据ETL中，如果已有Spark环境，可以使用该Connector完成最终落地。与Flink不同Spark多用于离线批处理，因此Spark Doris Connector常被应用于<strong>离线大数据管道</strong>的最后一步。比如一个Spark作业读取Hive几百GB数据，转换聚合后直接存入Doris作报表。相比先落盘再Broker Load，这种方式少了一步IO且更实时。需要确保Spark各任务并行数不要过高超出Doris FE承受范围，一般几百并发以内。</p>
</li>
<li><p><strong>DataX</strong> (批量场景)：前述DataX除了CDC同步，也可以跑一次性批量同步任务。例如配置一个DataX任务将MySQL某库的全表抽出，通过 Doris Writer 写入 Doris 实现迁移。DataX适合异构库（Oracle、Postgres等）的数据抽取，然后统一Load入 Doris。因此不少企业用 DataX+Doris 完成旧仓库替换。这种方案简单可靠，但由于DataX跑在单机/有限线程，速度可能比Spark/Flink慢。可通过部署分布式DataX或多任务并发提高速度。</p>
</li>
</ul>
<p><strong>实时与批量融合</strong>：Doris 还允许<strong>Mini-batch Merge</strong>，即同时进行批量导入与实时导入。比如每天离线导入历史数据文件，但在线部分数据通过Routine Load实时补充，Doris 能合并两部分数据（依赖Unique Key或delete条件确保不重复）。Doris 的事务机制能保证不同导入方式的数据一致可见。在选择导入方案时，建议根据数据源类型、数据规模、延迟要求来选：<strong>海量初始导入</strong> -&gt; Broker Load；<strong>周期批同步</strong> -&gt; Insert Select或DataX；<strong>实时流</strong> -&gt; RoutineLoad/Flink；<strong>API小流</strong> -&gt; Stream Load。Doris 导入的优势在于<strong>种类齐全</strong>且<strong>易于使用</strong>，基本覆盖了离线和实时的各类需求，这也是 Doris 相比某些竞品更“一站式”的体现。</p>
<h3 id="4-3-数据导出机制"><a href="#4-3-数据导出机制" class="headerlink" title="4.3 数据导出机制"></a>4.3 数据导出机制</h3><p>Apache Doris 除了导入，也提供将数据<strong>导出到外部系统</strong>的功能，方便与其他系统集成或备份。主要的导出方式有：</p>
<ul>
<li><p><strong>EXPORT 导出表/分区</strong>：Doris 的 EXPORT 命令可将指定表或分区的数据以文件形式导出到远端存储（如 HDFS、S3）。导出是异步执行的，由 FE 协调各 BE 读取其持有的数据分片并写出文件。当前支持文本（CSV）格式导出。用户通过 EXPORT 语句指定目标路径（如 HDFS目录、S3桶URL）和文件分隔符、压缩等参数。导出作业完成后，可在 Doris 查看其状态和生成文件列表。EXPORT 常用于<strong>全量备份</strong>或<strong>集群间数据迁移</strong>：先在源集群EXPORT到对象存储，再在目标集群通过 CREATE EXTERNAL TABLE 读取或 Broker Load 导入这些文件，实现数据转移。由于 Doris 表数据通常有多个副本，EXPORT 默认每个分片只从其一个副本导出以避免重复，且确保导出时刻的数据一致性（使用快照）。该功能对数据库运维很实用：可以随时dump出某表近期数据备份，或导出特定分区给其他系统使用。</p>
</li>
<li><p><strong>SELECT INTO OUTFILE</strong>：Doris 2.x 提供了 SELECT … INTO OUTFILE 语法，用于<strong>导出查询结果</strong>。它会将 SELECT 查询得到的结果集直接写入指定的外部存储，如HDFS路径或本地文件路径，格式支持CSV等。与 EXPORT 不同，它可以导出任意查询结果，而不仅是整表/分区数据，且为同步执行（查询成功即完成导出）。这对复杂报表需要离线保存的场景很有用。例如SELECT聚合统计结果写出CSV供业务下载。SELECT INTO OUTFILE 也可用于把 Doris 数据转存为Parquet/CSV供其他引擎使用（目前支持文本，Parquet支持正在开发）。需要注意写入HDFS/S3通常需要在 Doris 配置中设置好访问凭证。Select Outfile 让 Doris 自身承担了数据导出的工作，避免应用层从Doris拉取再写盘的流程，效率更高且简化架构。</p>
</li>
<li><p><strong>MySQL Dump</strong>：由于 Doris 兼容 MySQL 协议，使用mysqldump工具可以导出 Doris 表结构甚至数据。0.15版本开始 Doris 支持通过mysqldump导出表定义和数据为SQL文件，然后可导入到别的Doris集群或MySQL。但mysqldump对于大数据量效率较低，不如前述专有方式。</p>
</li>
<li><p><strong>数据外表方式</strong>：除了直接导出，Doris 3.0 开始支持直接将 Doris 内部表以外部表形式提供给Trino等系统使用。通过Trino Connector兼容框架，Trino/Presto可以访问 Doris 表，就像外部表查询一样。虽然这不是“导出”，但达到数据共享目的，不需要真正拷贝数据。这也是一种“Export On Demand”思路，让外部引擎直接查询 Doris 数据，实现即席的跨系统数据获取。</p>
</li>
</ul>
<p>总的来说，Doris 在数据导出上提供了必要的功能支持备份和集成，但相对导入手段要少一些。原因是很多情况下，Doris 被作为分析结果的终点，较少需要导出全量数据给其他系统。但在数据交换、备份方面 Doris 仍提供了 EXPORT/OUTFILE等手段，并可以借助其MySQL协议，用各种标准工具进行dump或增量同步（比如使用Debezium CDC Doris binlog）。在具体使用上，推荐用EXPORT完成周期备份，并配合操作系统/对象存储的生命周期策略，保障数据安全。同时监控导出任务，以便及时发现失败重试。通过灵活使用Doris的导入导出组合，企业可以构建数据在湖仓、DB和实时系统之间的<strong>双向流动</strong>，打通数据管道闭环。</p>
<h2 id="5-Doris-的-SQL-支持与索引特性"><a href="#5-Doris-的-SQL-支持与索引特性" class="headerlink" title="5. Doris 的 SQL 支持与索引特性"></a>5. Doris 的 SQL 支持与索引特性</h2><p>Apache Doris 致力于提供对<strong>标准 SQL 和丰富分析功能</strong>的支持，方便使用者以熟悉的 SQL 对数据进行查询和管理。同时，Doris 针对分析场景扩展了物化视图、索引等高级特性来提升查询性能。本节概述 Doris 的 SQL 兼容性、物化视图、二级索引（含Bitmap和倒排索引）机制等。</p>
<h3 id="5-1-SQL-兼容性与功能"><a href="#5-1-SQL-兼容性与功能" class="headerlink" title="5.1 SQL 兼容性与功能"></a>5.1 SQL 兼容性与功能</h3><p><strong>SQL 方言兼容</strong>方面，Doris 对 MySQL 语法具备高度兼容性，支持绝大多数 MySQL DDL、DML 和函数。用户可直接使用 MySQL 客户端或 BI 工具通过 JDBC/ODBC 访问 Doris，执行标准的 CREATE TABLE、SELECT、JOIN、GROUP BY 等查询。Doris 也支持大部分 ANSI SQL 标准，如子查询、视图、联合查询等。此外，还兼容部分 Hive 语法函数以方便大数据用户迁移。举例来说，Doris 支持 MySQL 的 IFNULL、DATE_FORMAT 等函数，也实现了 Hive 的 percentile_approx、regexp_replace 等函数，基本覆盖常见数据清洗和分析操作。对于分析常用的窗口函数 (OVER analytic functions)，Doris 1.1 起也已支持，如 RANK(), SUM() OVER(…) 等，可以用于计算排名、移动平均等高级分析。这使 Doris 可替代一部分传统数据仓库的功能，在OLAP场景中执行复杂统计计算。</p>
<p><strong>数据类型</strong>方面，Doris 支持丰富的标量类型：包括整型 (TINYINT~LARGEINT)、浮点、DECIMAL、高精度时间日期 (DATE, DATETIME, DATEV2等)、以及CHAR/VARCHAR/String 等文本类型。为了增强对半结构化数据的处理，Doris 在2.x和3.x版本陆续引入了 <strong>ARRAY, MAP, STRUCT, JSON, VARIANT</strong> 等复杂类型。例如可以创建 ARRAY<int> 列存储整数数组，或 JSON 列直接存储JSON文档。这些类型使 Doris 能直接存储和查询例如嵌套的属性数组、KV对等，新版本还提供相关函数如 <code>json_extract</code>、<code>array_join</code> 等辅助查询。这顺应了现代数据分析需要处理半结构化日志、嵌套数据的趋势。</int></p>
<p><strong>内建函数和操作</strong>方面，Doris 内置了超过400个常用函数，包括数学函数、字符串函数、日期函数和聚合函数等。例如 COUNT, SUM, AVG, MAX/MIN 等基础聚合，GROUP_CONCAT, APPROX_COUNT_DISTINCT, HLL_UNION_AGG 等高级聚合（HLL用于近似去重计数），以及like/regexp、case when等操作。Doris 还提供 Bitmap_union, Bitmap_count 等位图聚合函数，方便处理去重计数场景。特别值得一提，Doris 引入了 <strong>HyperLogLog (HLL)</strong> 数据类型和函数，用于海量数据的近似去重计数，性能远高于精确 count distinct。用户可定义HLL列存储HyperLogLog草图，在聚合表上直接用 HLL_UNION_AGG 聚合实现去重计数。这个功能在UV统计等场景非常有用。另外 Doris 还支持 <strong>Bitmap类型</strong>，可用于聚合模型下快速做并集、交集操作计算用户群，如 bitmap_union, bitmap_and 等函数。</p>
<p><strong>事务与一致性</strong>方面，Doris 并非OLTP数据库，但其<strong>导入事务</strong>保证了数据导入的一致性。每次导入（INSERT/Load）被视作一个原子事务，要么全部写入成功要么失败回滚，FE通过两阶段提交协议协调保证不同BE分片数据的同步可见。查询默认读取最新提交完成的版本，不会看到进行中的未提交数据。对于需要稳定视图的分析，Doris 支持 <strong>事务隔离</strong>：比如可以用 <code>ATOMICITY</code> 参数要求一批Insert atomicity。虽然 Doris 不支持多语句事务，但其<strong>单语句原子性</strong>和<strong>读已提交</strong>隔离足以应对绝大多数分析情境。再者 Doris 提供 <strong>Lockless Schema Change</strong>，即表结构变更（增加列、修改类型等）在线完成，对查询透明，不需要锁表等待。这通过版本管理和Schema变更任务异步执行实现，体现了在SQL管理上的友好。</p>
<p>综上，Doris 在 SQL 能力上几乎覆盖了传统数仓/数据库的大部分需求，且针对大数据分析场景有特殊优化的函数和类型。相比 ClickHouse 等，Doris 的SQL支持更加全面标准（如窗口函数CH在开源版迟迟不全），这也是很多用户选择 Doris 的原因之一。“上手容易，语法无学习成本”是 Doris 的卖点之一。京东的对比报告中就提到：“Doris SQL标准支持更好，功能更完善”。因此企业可以较轻松地将原有SQL逻辑迁移到 Doris，并利用其新增类型函数来简化某些计算。这为 Doris 在多变的业务分析场景中应用打下基础。</p>
<h3 id="5-2-物化视图-Materialized-View"><a href="#5-2-物化视图-Materialized-View" class="headerlink" title="5.2 物化视图 (Materialized View)"></a>5.2 物化视图 (Materialized View)</h3><p><strong>物化视图</strong>是在数据库中预先计算并存储的视图结果，是提升查询性能的重要手段。Doris 支持物化视图来加速常用聚合或复杂查询，可视为自动的“预汇总表”或“派生表”。Doris 的物化视图分为<strong>同步物化视图</strong>和<strong>异步物化视图</strong>两种模式：</p>
<ul>
<li><p><strong>同步物化视图</strong>（也称 <strong>Aggregate Rollup</strong>）: 这是 Doris 早期提供的物化视图机制。用户在创建表时或之后，通过 <code>CREATE MATERIALIZED VIEW</code> 语句定义基于单表的汇总视图。典型用法是在明细表上创建按某些维度聚合的视图，如对日志明细表创建按日期、地区聚合的MV。这种视图一经创建，系统会后台立即对已有数据计算填充视图，并在后续数据导入时<strong>同步维护</strong>视图数据。同步MV只能基于单表，不支持JOIN/LIMIT等复杂查询。它实际上是表的一个“Rollup索引”：Doris 查询优化器会自动匹配查询是否可使用已有MV加速，比如用户查询按date聚合，Doris检测到对应MV存在且新鲜，则改写查询从MV取数据而非扫描明细表。在实现上，早期Doris通过Rollup的概念，把一个物化视图当做主表的一个“索引”，存储结构上与普通表无异，FE维护MV与基表的列对应关系。增量数据导入时，BE会同步更新相关MV（需要对MV定义的聚合函数进行增量计算，如sum累加）。因此用户感知不到维护过程，一旦定义MV之后，查询速度就自动提升。例如创建一个按商品ID汇总销量的MV，则<code>SELECT item_id, SUM(sales) FROM table GROUP BY item_id</code>会被Doris改为直接扫描MV表返回结果，省去每次汇总计算。同步MV的缺点是<strong>模型受限</strong>：只支持单表，无JOIN；选择列也有限制（聚合列只能用SUM, MIN, MAX等有限函数）；而且MV更新会消耗额外资源。当数据更新频繁且MV复杂时，维护成本高。</p>
</li>
<li><p><strong>异步物化视图</strong>（Materialized <strong>Task</strong> View, 简称MTMV）: Doris 1.2+引入的新模式，允许定义任意SQL的物化视图，其数据由后台定期刷新而非实时同步。用户使用 <code>CREATE MATERIALIZED VIEW ... AS SELECT ...</code> 定义，其实质是创建一个内部表来存放视图结果，并登记一个刷新任务。刷新任务可以是定时的（比如每5分钟刷新一次）或手动触发。当刷新执行时，Doris内部跑一个 <code>INSERT OVERWRITE mv_table SELECT ...</code> 的作业，将最新源数据计算结果写入MV表。异步MV支持更复杂的计算逻辑，比如可以包含JOIN、子查询等，只要定义SQL符合要求即可。相当于用户自定义了一个批处理作业，让Doris定期执行以保持MV最新。查询时，Doris 查询优化器同样会根据查询模式匹配异步MV。对于定义相同的查询，优化器会自动改写为从MV表查询。如果MV的数据有稍许延迟（刷新周期内的新数据未包括），需要业务在意这一点。异步MV的优点是<strong>灵活强大</strong>：可以跨表JOIN汇总，也可以定义部分列表达式。它类似一个ETL作业在Doris内部完成。典型场景如：构建事实表和维表的预JOIN视图、或对JSON字段展开统计的视图等等。Doris 3.0 对异步MV的性能和使用做了改进，支持基于分区的增量刷新（如果MV定义与基表分区关联）。并提供可见的刷新调度管理，让用户可以暂停、手动触发刷新。异步MV有效缓解了同步MV模型受限的问题，使 Doris 可以替代部分人工ETL，将繁重计算提前，换取查询秒级响应。很多使用案例表明，合理设计MV可将一些复杂报表查询提速一个数量级以上。</p>
</li>
</ul>
<p>Doris 查询优化器能够<strong>自动选择合适的物化视图</strong>服务查询。当存在多个MV可用时，会基于数据行数等简单代价估计选最快的。例如有一个总汇总MV和一个更细粒度MV，Doris会挑粒度匹配查询条件的那个。物化视图极大提升了Doris对固定报表和常见查询的性能，是企业使用Doris进行多维分析时的利器。和ClickHouse等需要用户手工选择使用预计算表不同，Doris完全在查询层面对MV进行透明替换，<strong>无需修改原SQL</strong>。这一点降低了应用改造成本。在京东实践中，他们认为Doris的“物化视图自动聚合”是其分布式能力更强的一方面。需要注意使用MV需要占用存储空间和计算资源，应评估收益与成本平衡。一般来说，对那些<strong>计算量大又经常查询</strong>的汇总，可创建MV。Doris 未来也在计划引入更多自动化，比如根据查询日志推荐MV、或者自适应维护频率，以进一步提高易用性。</p>
<h3 id="5-3-二级索引与全文检索"><a href="#5-3-二级索引与全文检索" class="headerlink" title="5.3 二级索引与全文检索"></a>5.3 二级索引与全文检索</h3><p>在前文2.3节我们详细介绍了 Doris 存储层的各种索引，如前缀索引、ZoneMap、Bitmap、BloomFilter、倒排索引等。从数据库使用角度来看，这些除前缀索引外，都可视为<strong>二级索引</strong>（secondary index），即非主键的辅助索引结构。Doris 允许用户针对业务查询需要，手动创建一些索引来提升查询效率。</p>
<p><strong>Bitmap 索引</strong> (二级索引): Doris 早期版本支持用户通过 <code>CREATE INDEX ... USING BITMAP</code> 语句为某列创建位图索引。该索引适用于低基数维度列，用于等值和 [NOT] IN 查询加速。例如对性别列（只有M/F两值）建立Bitmap索引，则查询 WHERE gender=’F’ 可直接通过位图获得所有满足行号，不扫描整列。Doris 的位图索引实现基于 RoaringBitmap，将列所有不同值建立有序字典，并为每个值存储出现行号的压缩位图。因此其索引大小和值基数正相关，而与行数近似线性关系。实践经验表明，当列基数小于几十万且分布不极端倾斜时，Bitmap索引性价比高，可以换来10倍以上过滤性能提升。创建Bitmap索引后，Doris 查询优化器在解析SQL时如果发现相应列在谓词中，就会尝试使用该索引进行点查过滤。由于 Doris 的索引是Segment级别存储的，匹配过程也在BE扫描时完成，对用户透明。需要注意，在Aggregate模型/更新模型下，仅Key列允许建Bitmap索引，Duplicate/Primary模型则任意列都可以。而且一些类型如大文本字段不支持Bitmap。总的来说，Bitmap索引是在Doris成熟之前，用于补强点查性能的重要特性。</p>
<p><strong>倒排索引</strong>: Doris 2.0 后提供的全文检索/二级索引利器。在 2.x 版本中，倒排索引已被官方推荐为优先选择的索引方案，因其功能更全面。用户通过 <code>CREATE INDEX idx_name ON tbl(column) USING INVERTED</code> 创建之，对文本列可附加分词器选项。创建后，Doris 自动构建列值到行ID的映射文件存储。支持对字符串的 <strong>MATCH</strong> 查询、对数值日期的 =、&gt;、&lt; 等比较。Doris 倒排索引甚至可以在一个查询中结合多个倒排条件使用，发挥类似搜索引擎的布尔组合过滤功能。例如 <code>... WHERE col1 MATCH_ALL &quot;foo bar&quot; AND price &gt; 100</code>，Doris 可同时利用两个列的倒排索引取交集，加速获取行集合。这一点 Bitmap 很难做到。由于倒排索引文件独立存在，创建和删除不影响数据主文件，也可以对已有表无锁创建，比较灵活。从性能看，倒排索引对大表点查、子字符串查找的提速非常显著，使 Doris 具备了一定的全文检索能力。InfoQ 报道称，使用倒排索引后，一些文本模糊查询性能提升40倍。可以说，倒排索引让 Doris 在一些场景下可以替代 Elasticsearch 等检索引擎，用统一的SQL接口提供搜索分析服务。SelectDB 等商用版本甚至将其用于日志分析，显著降低运维和成本。不过倒排索引的代价是额外的存储和维护成本，数据更新时也需更新索引文件，这些由Doris内部处理。总体而言，对于搜索、复杂筛选等场景，倒排索引是非常值得开启的功能。</p>
<p><strong>NGram 索引</strong>：这是 Doris 3.0 引入的一种特殊倒排索引，用于支持中文等宽字符的全文检索。NGram 将文本切分成N字长的子串做索引，可更好支持通配符和模糊匹配。其适用场景及用法类似倒排索引，是对其补充（主要解决中文分词问题）。</p>
<p><strong>二级索引 vs 物化视图</strong>：两者都是加速查询，但思路不同。索引更像优化查询过滤的“短路径”，适用范围较广（可以对各种查询过滤生效），但不改变查询输出。物化视图则是预计算结果，适用于特定的聚合/计算。一般来说，如果瓶颈在于<strong>过滤IO</strong>，比如要从亿行中筛选出几千行，那么索引效果更好；如果瓶颈在<strong>计算</strong>，比如聚合运算量大，则物化视图更合适。当然两者可配合：如对物化视图表也能建索引来进一步加速过滤。在 Doris 实践中，位图索引曾用于典型的用户画像筛选场景，与物化视图共同满足多维筛选+指标计算的需求。但随倒排索引出现，很多用户开始直接用倒排进行用户标签筛选，因为其灵活性更好，可支持多条件组合匹配且更新代价低。</p>
<p>最后值得一提，Doris 还提供<strong>全局二级索引</strong>能力吗？相对传统OLTP数据库，Doris 没有典型的B+树索引。但通过上述 Bitmap/倒排等，已经覆盖了点查和范围查的大部分需求。对于Unique Key表，Doris 主键索引其实可以视为全局唯一索引；Duplicate表没有内置主键，但可以借助倒排索引实现等价的定位（如ID列建立倒排索引，效果如全局索引）。因此在Doris里，用户更多考虑是否需要创建<strong>辅助索引</strong>来优化筛选。实践经验是：<strong>高并发点查</strong>场景，优先考虑Unique Key + Rowstore方案（主键直接查）；<strong>维度筛选</strong>场景，使用倒排索引；<strong>Cube类大量组合筛选</strong>，物化视图或预聚合（过多组合无法逐个索引覆盖）。Doris 提供的多种索引机制，为不同查询模式都准备了武器。这种多样性在其他同类产品中并不多见（ClickHouse没有内置倒排，只有skip index；StarRocks功能类似Doris）。因此Doris的索引方案在使用中应根据具体需求选型，以达到最佳性价比。</p>
<h2 id="6-高并发低延迟场景优化策略"><a href="#6-高并发低延迟场景优化策略" class="headerlink" title="6. 高并发低延迟场景优化策略"></a>6. 高并发低延迟场景优化策略</h2><p>在实时分析场景中，Doris 常常需要同时支撑大量查询请求（高并发）并保证每个请求的快速响应（低延迟）。为此，Doris 从架构和实现上都设计了多种优化策略来提升并发处理能力与降低延迟，包括缓存设计、调度控制和内存管理等方面。</p>
<h3 id="6-1-缓存设计"><a href="#6-1-缓存设计" class="headerlink" title="6.1 缓存设计"></a>6.1 缓存设计</h3><p><strong>缓存 (Cache)</strong> 是提升查询响应速度的重要手段。Doris 内部主要有三层缓存机制：</p>
<ul>
<li><p><strong>数据页缓存 (Page Cache)<strong>：如前所述，Doris BE 对读取过的列数据页采用LRU缓存。当相同Segment的某页再次被请求时，可直接从内存返回，避免磁盘IO。这对</strong>热点数据</strong>或重复扫描非常有效。例如多个查询都涉及最近一小时的数据，那么该时段的Segment页第一次加载后会驻留缓存，后续查询几乎无需磁盘读取即可获取。这种缓存是<strong>分列</strong>的，因为Doris以列为单位缓存每个页。相比传统行存的页缓存更细粒度，也更节省内存。不过当查询涉及许多列或全表扫描，缓存命中率会下降。Page Cache 的大小可以通过BE配置 mem_limit 等参数间接控制，它和数据存储共享进程内存，由内存管理模块统筹（见6.3节）。测试表明，适当加大 Page Cache 可以有效提升点查和重复报表查询性能，但对于完全随机的大规模分析，收益有限，需要配合其他手段。</p>
</li>
<li><p><strong>行缓存 (Row Cache)<strong>：针对Unique Key表启用了行存储（<code>store_row_column=true</code>）的情况，Doris 提供单独的行缓存。因为此时行数据不再一列一列地独立缓存了，需要整体缓存多列组合的行块，否则普通Page Cache可能频繁驱逐行block数据。Row Cache 复用BE的LRU机制，但缓存的是组合行数据，默认上限占BE内存的20%。Row Cache 能极大提高</strong>点查询</strong>的命中率。例如Primary Key查询，由于行block被缓存，下次查相同键就无需甚至绕过存储层直接命中缓存返回。此外Row Cache避免了列式缓存中某大查询把所有列页都换入导致行数据缓存被挤出的情况。需要通过BE配置启用Row Cache（默认关闭）。对于那种经常查询同一批主键详情的应用，开启Row Cache效果明显。另一方面，如果工作负载主要是扫描分析，则Row Cache意义不大，关闭可节省内存。一般建议对<strong>QPS很高的点查服务</strong>开启Row Cache，配合Unique Key模型达到KV缓存近似的效果。</p>
</li>
<li><p><strong>查询结果缓存 (Query Cache)<strong>：FE 层面的结果缓存，包括SQL Cache和Partition Cache。SQL Cache存储最近一段时间执行过的</strong>完全相同</strong>查询的结果，命中则直接返回，不执行查询逻辑。Partition Cache缓存的是按分区划分的查询结果片段，支持不同但共享部分分区的查询复用。FE 对每个查询解析后，会判断是否启用缓存以及缓存命中情况。如果命中，直接由FE返回缓存结果。否则正常下发执行，并在返回前将结果写入缓存（同时关联上对应表分区的版本号）。当源表分区有数据更新（版本变化）时，FE会使相关缓存项失效，以保证后续查询拿不到旧结果。结果缓存对<strong>报表类重复查询</strong>意义重大。例如某日报表每小时刷新一次，中间多次相同查询可直接命中缓存，提高并发效率并降低BE负载。Partition Cache 甚至允许类似“本周数据”和“本月数据”这种查询部分重叠时复用共同部分缓存，提高缓存利用率。据官方说明，Doris 结果缓存优先保证数据一致，其次才精细划分颗粒度以增加命中。为了避免无谓的缓存占用，FE限定只有结果行数低于 <code>cache_result_max_row_count</code> 且数据量低于 <code>cache_result_max_data_size</code> 的查询才缓存（默认3000行或30MB以内）。因此Cache多用于汇总/小结果集报表，而非海量明细导出。用户可通过FE配置调整这些阈值。Doris 结果缓存是内置的无需额外组件方案，相比应用层套Redis等缓存，具备<strong>数据一致性自动维护</strong>和<strong>高命中率</strong>（粒度细）优势。很多BI看板场景用上Partition Cache后，即使高并发刷新也不会给后台集群造成太大压力。</p>
</li>
</ul>
<p>总的来说，Doris 缓存体系覆盖了<strong>存储访问</strong>和<strong>查询结果</strong>两个层面，从源头减少重复IO和重复计算。在高并发场景下，这意味着大部分热点请求可以在内存级别解决，不会对磁盘或CPU造成线性增长的负载。如某互联网公司将Doris用于实时服务，利用Row Cache+Result Cache，实现了百万级QPS下99%请求都走内存返回，后端集群资源占用稳定。需要注意缓存并非万能：在全新查询或扫描大比例数据时，缓存帮不上忙。因此结合6.2节的调度控制与6.3节的内存管理，确保系统在cache未命中时也能平稳处理是关键。而缓存主要保证了<strong>局部热门</strong>或<strong>重复</strong>查询的极致性能表现，这是提升用户体验的利器。</p>
<h3 id="6-2-并发调度与工作负载管理"><a href="#6-2-并发调度与工作负载管理" class="headerlink" title="6.2 并发调度与工作负载管理"></a>6.2 并发调度与工作负载管理</h3><p>当大量查询同时到来时，合理的<strong>并发控制与调度</strong>机制可以防止系统过载并保证重要查询的响应时间。Doris 引入了<strong>Workload Group</strong>（工作负载组）机制来管理多租户和并发资源。通过工作负载组，管理员可以为不同类型的查询分配资源配额和并发限制，从而实现隔离和熔断。</p>
<p><strong>工作负载组</strong>：可以理解为 Doris 内部的“资源池”或“队列”。通过 <code>CREATE WORKLOAD GROUP</code> 定义，指定该组的最大并发数、队列长度、超时时间等参数。还可设置该组的 CPU 配额（cpu_share）和内存上限（memory_limit）百分比。将不同会话或用户映射到不同组后，他们提交的查询就受到相应限制。例如定义一个 groupA 并发上限10、队列长度20、超时3秒。当 groupA 已有10个查询在跑，第11个查询会进入队列等待，若超过3秒还未开始则被取消。这样确保同时运行的查询不超过10个，防止资源过度竞争。再如为ETL组设置较低cpu_share，BI组较高，则当两类查询并发时BI组会优先调度更多CPU。</p>
<p><strong>默认组</strong>：Doris 默认有个名为 <code>normal</code> 的组。初始配置是无限并发（max_concurrency=最大int）、无排队（max_queue_size=0），表示不限制并发。在未配置工作组时，所有查询都跑在normal组中，相当于无控制。要启用并发控制，必须创建自定义组并将对应连接设置组标签。可以通过 <code>ALTER USER user1 SET workload_group=&#39;groupA&#39;</code> 或在连接URL中指定。然而目前 Doris 没有复杂的自动路由逻辑，需要用户或应用自己选择在哪个组跑查询。例如可以将ETL脚本账号设置进 etl组，将前端报表账号进 bi组。</p>
<p><strong>排队和拒绝</strong>：当并发超限时，新查询将按配置进入等待队列。如果队列也满，新查询直接被拒绝（FE会报错告知并发过高）。排队等待超时未能执行的查询，也会被取消返回超时错误。这些机制相当于在高压下为系统加了一道保险，宁可让部分请求延后或失败，也不让系统陷入崩溃或极度缓慢。对于OLAP场景来说，这种“<strong>软隔离</strong>”非常必要，否则少数大查询可能拖垮整个服务。</p>
<p><strong>多FE情况下</strong>需注意，当前 Doris 的队列计数是<strong>每个FE单独</strong>的，没有全局协调。也就是说，如果配置某组最大并发1，而有3个FE，那么可能每个FE上各跑1个查询，总共还是3个在跑。因此要实现全局限制，需要确保请求都走同一个FE或降低各FE配置。未来社区可能改进为全局感知队列。多FE环境通常通过负载均衡分发请求，因此每个FE上看到的并发只是总并发的1/N，需据此调整参数。</p>
<p><strong>查询优先级</strong>：Doris 没有显式的查询优先级参数，但通过将重要查询放在一个高cpu_share、高内存组且限制它的并发，可以保证它们资源充足且不被阻塞。对于低优先级批处理，则可限并发低、甚至让它排长队或者在夜间单独跑。</p>
<p><strong>查询熔断</strong>：除了Workload Group之外，Doris 还有<strong>查询断路 (Query Circuit Breaker)</strong> 机制，当单个查询消耗资源过多时强制取消。FE和BE都有监控，如果检测到查询使用内存过大接近上限、运行时间过长，或者Scan行数过多超过阈值，会主动终止查询释放资源。这防止了“劣质查询”长期霸占系统。Circuit Breaker 可以在fe.conf/be.conf中配置启用及阈值。</p>
<p><strong>并发调度</strong>：Doris BE 在 Pipeline 执行基础上也做了<strong>本地并发调度</strong>，如前述 Pipeline 会自适应调整线程数避免上下游不平衡。BE还有<strong>Scan线程池</strong>控制每节点并发扫描线程数量（scan_thread_num默认16）。可通过调大该值提高单节点扫描吞吐，但过大可能争抢CPU。Workload Group也提供 per-group的 scan_thread_num 控制，以免一个组耗尽IO线程。此外 Doris 的compaction等后台任务也有独立线程池并限制并发，以防导入和查询互相影响。</p>
<p>通过以上机制，Doris 能够做到<strong>在高并发下的平稳服务</strong>：当请求量超过处理能力时，有序排队或失败，不至于拖慢所有查询；对不同用户/任务隔离资源，互不抢占；对异常繁重查询及时中止。实践证明，这让 Doris 比较适合作为<strong>多租户共享集群</strong>使用。不少公司内部提供Doris分析服务，就依赖Workload Group限制各团队资源，用Quota机制来防止少数人耗尽集群。相比之下，ClickHouse 需要CHProxy等外部组件+用户自行写脚本才能做到类似控制。Doris 原生支持多租户和并发管理无疑降低了运维复杂度。总结说，Doris 的并发调度体系保证了<strong>公平和稳定</strong>：公平分配资源，稳定应对高峰，保障关键查询及时完成。</p>
<h3 id="6-3-内存管理与溢出保护"><a href="#6-3-内存管理与溢出保护" class="headerlink" title="6.3 内存管理与溢出保护"></a>6.3 内存管理与溢出保护</h3><p>内存是影响查询稳定性的关键资源。Doris 在内存管理上采取了多层控制和溢出(Spill)机制，以防止 OOM 崩溃并提升大查询的处理能力。</p>
<p><strong>内存分级控制</strong>：Doris 将内存使用分为三个层级进行限制：</p>
<ul>
<li><p><strong>BE进程级</strong>：通过 be.conf 的 <code>mem_limit</code> 参数设置每个BE进程的内存上限（如80%机器内存）。BE每次申请内存都会通过 Doris Allocator 先检查全局已用量是否超过 mem_limit，如是则拒绝申请。一旦BE整体内存达到上限，Doris会立即取消当前请求内存的查询并触发一些自救措施（如释放cache）。这个limit确保BE不至于耗尽系统内存引发OS级 OOM，被操作系统杀掉。</p>
</li>
<li><p><strong>Workload Group级</strong>：每个工作组可以配置 <code>memory_limit</code>，默认为30%。表示该组的所有查询最多使用BE总内存的30%（soft limit）。当组内查询超过此额度且 enable_memory_overcommit=false，则超出部分将被触发取消或spill。如果 enable_memory_overcommit=true（默认），则组内可超限用内存，但一旦BE总体内存紧张，会优先取消超限组的查询。这样分组之间形成软隔离：大查询占满自己的组额度后，不会无限挤占别的组资源。对于需要更严格隔离的场景，可关掉overcommit，让组变硬上限。</p>
</li>
<li><p><strong>Query 查询级</strong>：每个查询有 exec_mem_limit 会话变量，默认2GB。表示单个查询能使用的最大内存。如某SQL占用超过2GB，会按配置选择要么溢出部分数据到磁盘（如果开启Spill）、要么直接取消查询。enable_mem_overcommit 控制查询是否可超限，默认true即允许超2GB但风险在于如果全局内存不足就kill掉。为了避免查询毫无限制吃内存，可以调低exec_mem_limit或设置 overcommit=false 强制其用完2GB就spill或终止。这对用户不友好的大SQL起到保护作用。</p>
</li>
</ul>
<p>以上三级限制共同作用：先保证进程总内存不爆，然后组内分配比例，最后单查询不至于过大。它们之间是<strong>包含关系</strong>：组限制不能突破BE mem_limit，总查询内存不能突破组limit（硬隔离时）。Doris 通过 Memory Tracker 在每个算子、每个Group都跟踪当前内存使用，动态判断是否超阈值。当BE内存高于 high_watermark（默认90%）时，会尝试释放cache等腾空间；当达到mem_limit则紧急清退查询。Doris 在K8s或Cgroup中运行时也能读取容器内存限额自动调整mem_limit。</p>
<p><strong>Spill to Disk</strong>：Doris 2.0+引入了<strong>内存溢出至磁盘</strong>的功能，在查询需要内存超限时，将部分中间数据暂存磁盘以降低内存占用。Spill 机制大大提高了 Doris 处理超大数据集或复杂查询的能力，使一些原本会OOM失败的查询能够完成，只是速度慢一些。目前支持Spill的算子有Hash Join、Aggregation、Sort、CTE（公用表表达式）。触发流程：当某操作需要申请内存块但Memory Manager判定会超limit，则返回失败信号给该算子。算子收到信号，主动暂停，选择自己可spill的最大数据结构，将其写出磁盘临时文件，然后释放这部分内存，再恢复执行。例如Hash Join构建哈希表占满内存，则将部分哈希分区溢出磁盘，仅保留一部分在内存处理，待处理完再读回溢出部分继续。排序类似，将超内存的行批写入外部临时文件分段排序，最后多路归并。Spill后查询耗时可能大幅上升（因为磁盘IO慢几个数量级），因此 Doris 建议在开启Spill情况下调大 query_timeout 来防止尚未完成就超时。另外Spill会产生大量磁盘读写，官方建议配置专用SSD目录给溢出文件，避免干扰正常数据读写。Spill功能默认关闭，需要设置 session变量或BE配置打开。目前它相当于一种“安全阀”：平时不用，当数据超内存时才启用。随着用户需求，未来版本可能让Spill更自动化。</p>
<p><strong>内存分配优化</strong>：Doris 针对向量化执行做了内存池和复用优化，减少频繁malloc/free。此外 Doris 使用TCMalloc作为内存分配器，具有较好的多线程性能。Doris 在 1.x 版本经历过一些内存泄漏bug，后续都通过Memory Tracker完善避免。2.x加入<strong>内存碎片监控</strong>，能识别内存使用异常的查询并警告，这在社区文章“告别 OOM”中有描述。</p>
<p>总的来说，通过<strong>内存限额+Spill</strong>，Doris 做到了<strong>不崩溃</strong>、<strong>能落盘</strong>。从经验看，在合适配置下，Doris 很少出现进程被OOM kill的情况，即使遇到超大查询，一方面Workload组限制它并发，另一方面Spill能保证完成。因此 Doris 变得可以处理过去需要分批的任务。不过使用Spill的查询耗时会大增，用户还是应尽量优化SQL、增加内存或预计算来避免频繁spill。内存管理策略让Doris在资源紧张时选择牺牲部分性能换取系统存活，符合“分析业务可等数据、不能接受系统挂掉”的要求。</p>
<p>综上，Doris 在高并发、低延迟保障方面建立了一套完善的策略：<strong>用缓存换性能</strong>、<strong>用队列稳住吞吐</strong>、<strong>用限额和磁盘守住内存底线</strong>。这套组合拳让 Doris 能够在复杂混合作业环境中依然保持较高性能和稳定性。相比一些只关注高性能但缺少保护机制的方案，Doris 更注重整体服务质量，这也使其成为企业级选型时偏好的原因之一。</p>
<h2 id="7-与主流生态系统的对接能力"><a href="#7-与主流生态系统的对接能力" class="headerlink" title="7. 与主流生态系统的对接能力"></a>7. 与主流生态系统的对接能力</h2><p>Apache Doris 拥有良好的<strong>生态集成</strong>能力，可以无缝对接主流的大数据和数据仓库系统，包括 Flink、Spark、Hive、Kafka 等。在现代数据架构中，Doris 往往扮演实时数仓或分析加速引擎的角色，需要从各种上游获取数据、与各种工具协作输出分析结果。Doris 官方提供了一系列 <strong>Connector</strong> 和 <strong>外部表</strong> 功能，实现与这些生态的融合。</p>
<h3 id="7-1-Flink-集成"><a href="#7-1-Flink-集成" class="headerlink" title="7.1 Flink 集成"></a>7.1 Flink 集成</h3><p><strong>Flink Doris Connector</strong> 是 Doris 面向 Apache Flink 平台的官方连接器，支持 Flink 从 Doris 读和向 Doris 写。在 Flink 作业中，使用 Doris Connector 可以像操作常规表一样对 Doris 表进行查询或插入。具体包括：</p>
<ul>
<li><strong>Doris Sink</strong>：允许 Flink 将实时流计算结果写入 Doris。典型用法是在 Flink SQL 中定义一个 Doris 表（通过 CREATE TABLE DDL，使用 Doris Connector），然后使用 <code>INSERT INTO doris_table SELECT ...</code> 将流数据插入 Doris。Doris Sink 内部通过小批次缓冲 + Stream Load 接口将数据加载，保证 Exactly-Once 语义。对于Flink DataStream API，也可以使用 DorisSinkFunction 达到同样效果。Flink Doris Connector 目前支持 Flink 1.11 - 1.20 等多个版本。有了它，可以方便实现<strong>流式ETL</strong>：例如用 Flink 从 Kafka 读取点击流，经过清洗聚合，再写入 Doris 表，实现实时指标更新。</li>
<li><strong>Doris Source</strong>：Connector 也提供 Doris 为 Flink 数据源的功能。用户可以用 Doris Connector读取 Doris 表的数据进入 Flink。在 Flink SQL 中，这体现为 Doris表可参与JOIN或Select查询。Doris Source 支持批处理读取所有数据或基于主键增量读取。如果结合 Flink CDC，可以构建从 Doris 导出数据到下游系统的管道。但相对Sink用例，Source用例较少，因为Doris更多作为终点。值得一提，Doris 2.1 引入了 Arrow Flight 协议支持，未来 Flink 有望通过 Arrow Flight 来更高效地批量读取 Doris。</li>
</ul>
<p><strong>Flink+CDC</strong>：Doris 官方还提供了 Flink CDC 与 Doris 结合的方案。如使用 Flink CDC 捕获 MySQL binlog，更改流经 Flink转换后用 Doris Connector下沉 Doris。这样实现数据库 -&gt; Doris 的实时同步。另外 Doris 3.0 开始计划支持 CCR(Change Data Capture)功能，或将简化与Flink CDC的集成。</p>
<p>Flink 与 Doris 的集成使得 Doris 可以很好融入现代流批一体数据平台。Flink 擅长数据处理，Doris 擅长查询分析。两者结合，可以实现<strong>实时数据流水线</strong>：数据从来源 (OLTP, MQ等) -&gt; Flink实时处理 -&gt; Doris query服务。而且Flink Connector维护起来比写自定义程序低成本，已被许多用户采用。</p>
<h3 id="7-2-Spark-集成"><a href="#7-2-Spark-集成" class="headerlink" title="7.2 Spark 集成"></a>7.2 Spark 集成</h3><p><strong>Spark Doris Connector</strong> 用于 Apache Spark 环境下同 Doris 交互。Spark Connector允许 Spark SQL/DataFrame 直接读写 Doris：</p>
<ul>
<li><strong>Doris Spark Sink</strong>：Spark可以通过 <code>df.write.format(&quot;doris&quot;)...save()</code> 将一个DataFrame保存到 Doris 表。其内部实现类似Flink，通过并行任务批量调用 Doris Stream Load 导入。这适用于批处理场景，将Spark计算结果（例如机器学习特征、离线ETL输出）批量加载进 Doris。例如每天的离线模型结果经Spark计算后，用Spark Connector导入Doris供查询。</li>
<li><strong>Doris Spark Source</strong>：Spark SQL可通过 <code>spark.read.format(&quot;doris&quot;)</code> 读 Doris 表为DataFrame。Connector会将Doris分片数据并发读出，利用Doris自身的并行查询能力。不过在Spark端，只能走JDBC方式pull数据或者ScanNode API，因此效率上不如直接在Doris查。但对于Spark要处理Doris数据的场景，这比先导出CSV再读要方便快速。</li>
</ul>
<p>Spark Connector与Flink Connector类似，配置 Doris FE地址、账号、表名等即可。腾讯云的文档和SelectDB都有对Spark Connector的介绍。使用Spark Connector需注意不要过度并发（默认每Spark分区一个Stream Load作业，大表可能瞬时创建大量作业给FE压力），通常Spark partitions数量应适当小于Doris BE总数以平衡性能和负载。</p>
<p>Spark与Doris的结合多用于<strong>离线+在线</strong>的融合：Spark作为全量离线数仓，Doris作为在线分析引擎。Spark Connector让两者数据流动顺畅。例如运营每天用Spark跑复杂ETL归总，再把结果写Doris给业务部门查询报告；或Doris的部分维度数据需要Spark拿去做进一步机器学习训练等。总之Spark生态的连接保证了Doris不会成为一个数据孤岛，而能参与企业大数据流水线。</p>
<h3 id="7-3-Hive-与数据湖集成"><a href="#7-3-Hive-与数据湖集成" class="headerlink" title="7.3 Hive 与数据湖集成"></a>7.3 Hive 与数据湖集成</h3><p>Doris 非常强调与<strong>数据湖/仓</strong>的融合，提供了<strong>多种方式对接 Hive/Iceberg/Hudi 等</strong>。主要通过**外部表 (Catalog)**机制：</p>
<ul>
<li><p><strong>Hive Catalog</strong>：Doris 可以连接 Hive Metastore，把 Hive 数仓中的库表映射为 Doris 外部表。通过命令 <code>CREATE CATALOG hive1 PROPERTIES(&quot;type&quot;=&quot;hms&quot;,&quot;hive.metastore.uris&quot;=&quot;thrift://...&quot;)</code> 连接 Hive Meta后，<code>USE CATALOG hive1; SHOW DATABASES;</code> 就能看到Hive里的库表列表。Hive Catalog 下的表可直接在 Doris 中查询，就像普通表一样：<code>SELECT * FROM hive1.db.table</code>。Doris 会主动识别 Hive表分区，进行分区裁剪，并从HDFS上读取Parquet/ORC等文件。这本质上让 Doris 成为一个查询引擎，可以查询 Hive 离线数据而无需导入（类似Presto功能）。测试表明，对Hive数仓上的典型查询 Doris 可以提升数倍性能。另外 Doris 2.1.3 起甚至支持通过 Doris 对 Hive 表做 DML：可以用 Doris 创建Hive表、写入数据。这是通过Hive Catalog集成Spark引擎实现的，但透明给用户。这意味着Doris不仅能读Hive，还能写Hive，让Doris与Hive的数据互通更紧密。</p>
</li>
<li><p><strong>Iceberg/Hudi</strong>：Doris 3.0 之前支持通过 Hive Metastore 访问Iceberg/Hudi（因为它们的meta也注册在HMS，可当Hive表读）。3.0 后 Doris 直接支持 Iceberg Catalog，连接Hive或GlueCatalog读取Iceberg表原生元数据，包含ACID快照等。同时 Doris 3.0 提供<strong>写回 Iceberg/Hudi</strong>的能力。用户可以在 Doris 里创建Iceberg表，然后通过INSERT写数据，由Doris生成Parquet并提交Iceberg元数据。这实现了<strong>湖仓一体</strong>的双向打通：Doris 可查询和加速数据湖数据，又可以将加工结果回存数据湖。这对于希望用Doris统一处理但又要存储在湖格式的企业很有吸引力。典型场景如用Doris计算完结果写回Iceberg供Spark离线再利用，或Doris作为ETL引擎加工湖中数据再存回湖形成新的数据集。</p>
</li>
<li><p><strong>JDBC Catalog</strong>：Doris 也支持通过JDBC读取 MySQL/Postgres 等外部表。这对小规模维表或者需要join少量维度数据很有用。Doris 可通过JDBC Catalog定义连接一个MySQL，然后把MySQL的某表当外部表使用。查询时 Doris 会根据需要<strong>将条件下推</strong>到MySQL去执行，然后取回来数据join。这有点类似Trino的MySQL connector。但由于OLTP数据库不适合scan大数据，所以此用法仅限小表。</p>
</li>
<li><p><strong>其他数据源</strong>：Doris 3.0 引入<strong>Trino Connector兼容框架</strong>。通过这个架构，Doris 可以复用Trino现有Connector去连更多源。比如 Delta Lake、Kudu、BigQuery、Kafka 这些Trino已有connector的，Doris 3.0 已经完成适配。也就是说，现在 Doris 理论上可以当Trino用，连几十种不同的数据源统一查询。例如 Doris 可通过TrinoConnector直接查询Delta Lake的数据，或直接从Kafka读数据作为表。这些功能拓展了 Doris 的生态版图，使其成为真正的多源分析平台。</p>
</li>
</ul>
<p><strong>Kafka</strong>：除了 Routine Load（Kafka-&gt;Doris）外，Doris 3.0 通过 Kafka Connector 也可以 <strong>查询Kafka</strong>。将Kafka看作表，每个partition当成文件，这对简单实时监控Kafka数据等可能有用。但更主要用途是通过连接 Kafka 实现<strong>实时联邦分析</strong>：例如JOIN一个Kafka流表和一个Doris维表。在Presto/Trino里很常见，现在Doris也能做到，提升了统一分析的能力。</p>
<p><strong>Doris 与 BI</strong>：Doris 兼容MySQL协议，绝大多数BI工具（Tableau、PowerBI、Superset等）可以直接用MySQL驱动连接 Doris 做报表。这实际上也是生态对接，Doris 在BI场景非常友好，无需专门适配。</p>
<p>总结来说，Doris 的生态集成理念是**“一点接入，多处互通”<strong>。通过外部表/Catalog，Doris 打通了与Hadoop数据湖、云数据仓库、消息系统、OLTP数据库的界限。用户可以用 Doris 作为一个中央查询引擎，跨库跨源拉通分析，这减少了数据搬迁成本，简化了架构。在Lakehouse趋势下，Doris 的这种融合能力尤为重要：真正做到既能做实时数仓，又能查询数据湖数据，实现湖仓融合。Doris 社区的未来路线也强调继续增强“统一分析”方向，包括更多Connector、更好的外表性能等。从某种程度上讲，Doris 正朝着成为像Trino那样的联邦查询引擎+自身存储引擎二合一的形态发展，为用户提供</strong>一站式的分析体验**。</p>
<h2 id="8-同类产品对比分析"><a href="#8-同类产品对比分析" class="headerlink" title="8. 同类产品对比分析"></a>8. 同类产品对比分析</h2><p>Apache Doris 与其他几款流行的分析型数据库/引擎（如 ClickHouse、StarRocks、Greenplum、Presto/Trino）在性能、架构、易用性等方面各有特点。下面从主要维度对 Doris 进行横向对比。</p>
<h3 id="8-1-Doris-vs-ClickHouse"><a href="#8-1-Doris-vs-ClickHouse" class="headerlink" title="8.1 Doris vs ClickHouse"></a>8.1 Doris vs ClickHouse</h3><p><strong>ClickHouse</strong> 是由Yandex开源的高性能OLAP引擎，以极快的单机查询性能著称。两者比较：</p>
<ul>
<li><p><strong>架构</strong>：Doris 采用 FE/BE 两层架构，有集中式元数据管理和多副本保证一致性；ClickHouse 则没有独立元数据节点，每个节点独立执行查询，通过分布式表机制进行协同，元数据（表结构）通过配置文件或Zookeeper同步。CH常依赖 ZooKeeper 来管理副本和分布式DDL，所以部署需要 CHServer + ZK，并常配CH-Proxy辅助。而 Doris 部署简单，只需FE/BE进程，内置元数据复制，不依赖外部组件。因此 Doris 运维较容易，弹性扩缩容方便（Add Backend命令即可自动扩容），CH扩容需手动改配置分发。</p>
</li>
<li><p><strong>性能</strong>：ClickHouse 在许多场景下性能更佳。尤其在<strong>单表大查询</strong>上，CH的向量化执行引擎和高度优化代码通常快于 Doris。CH 核心团队在计算机体系结构上很有造诣，针对硬件优化明显。测试显示，相同数据量下，CH的导入速度和单表查询速度都略胜 Doris。CH 对复杂查询（多表JOIN、大聚合）也在改进，但 Doris 在这方面的SQL优化更成熟，JOIN性能往往更稳定。综合评价：<strong>单节点极致性能</strong>CH更强，但<strong>多表复杂查询</strong>Doris可能有优势（因为CH以往对JOIN支持有限，Doris有CBO/MV等优化）。</p>
</li>
<li><p><strong>功能</strong>：CH 功能非常丰富，有各种表引擎（MergeTree, ReplacingMergeTree等几十种）、大量内置函数和物化视图能力，也支持嵌套数据类型等。不过 CH 在<strong>事务一致性</strong>、<strong>标准SQL</strong>方面不如 Doris。比如CH没有事务导入概念，分布式表写入需要用户自行避免重复。Doris支持事务和幂等导入。CH 列类型多样，但窗口函数、部分标准SQL在开源版较晚支持。Doris 提供MySQL风格语法，对开发更友好。CH 在<strong>配置优化参数</strong>上非常多，可以细调性能，但这也增加了使用复杂度。Doris 参数较少，更自动。</p>
</li>
<li><p><strong>可维护性</strong>：Doris 的运维简单性被认为优于CH。比如 Doris 节点故障自动恢复副本、增加节点自动rebalance。CH 节点故障后副本恢复复杂，zookeeper同步延迟可能导致数据不一致，需要人工干预。CH 在多租户/权限上也弱一些，虽然支持用户Quota但细粒度上Doris略胜。</p>
</li>
<li><p><strong>应用场景</strong>：<strong>重定制开发、大规模</strong>：CH更适合。京东调研建议“数据规模超大且愿意投入研发深度优化的选CH”。<strong>一站式解决方案、少开发</strong>：选 Doris，因为 Doris 开箱即用特性多。CH 起源于流量日志分析，Doris 源自广告报表分析。两者很多场景可互换，但CH需要更多专家调优才能发挥最佳性能，而 Doris 使用门槛低、运维成本低。CH劣势在于“使用门槛高，运维成本高，分布式能力弱”，Doris劣势是“性能差一些，可靠性差一些”。所以企业若有强大技术团队追求极致性能，会青睐CH；而更多企业倾向Doris的易用性和较全功能。</p>
</li>
<li><p><strong>生态与社区</strong>：CH 社区全球用户多，在实时分析领域名气大；Doris 虽起步晚，但Apache顶级项目带来了快速增长的社区（600+贡献者）。Doris 以中国公司贡献为主，CH 国际化更好。但国内近年Doris用户也激增，案例丰富。商业支持方面，CH有Yandex和ClickHouse Inc.，Doris有国内多家厂商（如戴文VeloxDB、SelectDB）提供商业版或云服务，各有选择。</p>
</li>
</ul>
<h3 id="8-2-Doris-vs-StarRocks"><a href="#8-2-Doris-vs-StarRocks" class="headerlink" title="8.2 Doris vs StarRocks"></a>8.2 Doris vs StarRocks</h3><p><strong>StarRocks</strong> 是 2020 年自 Doris 社区分拆出的新项目，由原 Doris 开发团队创立。StarRocks 可以看作 Doris 的进化版，两者有相同基因但在部分设计上 diverge：</p>
<ul>
<li><p><strong>源代码关系</strong>：StarRocks 最初基于 Doris 0.14 版本 fork，之后进行大量重构优化。许多改进后来也贡献回 Doris（如向量化、Primary Key模型等），但StarRocks推进更快，功能更新频率更高。</p>
</li>
<li><p><strong>性能</strong>：StarRocks 诞生目标就是更高性能、更极致的分析引擎。SR 在<strong>向量化</strong>引擎更早成熟，<strong>Cost-Based优化</strong>更完善，引入<strong>Morsel-driven pipeline</strong>等先进执行技术，使其在部分查询上快于Doris。一些内部压测显示单表查询StarRocks略优，但二次查询（cache命中或warm cache）两者相差不大。多表查询情况下，大部分场景Doris与StarRocks性能相当，只有极少场景Doris逊于SR。总的来说，StarRocks 的研发投入大，在性能上保持领先半步，但 Doris 2.x/3.x 逐步缩小差距。部分场景双方打成平手。</p>
</li>
<li><p><strong>功能</strong>：StarRocks 对<strong>更新模型、Lakehouse</strong>等商业需求反应更快。它率先支持主键更新、外部表，开源后又快速迭代物化视图、向量搜索功能等。Doris 跟进稍慢但全面。不过StarRocks有一些闭源或商业特性（早期一些增强仅企业版提供，后来逐步开源）。SelectDB作为StarRocks的商业云服务，也提供更多企业特性如审计UI等。反之，Doris 作为社区项目更开放透明。StarRocks目前也是Apache License开源，但Doris社区有人批评SR“没有回馈社区”。两者在基本功能上类似，但StarRocks往往更注重<strong>云原生</strong>：如他们推Storage-Compute分离早于Doris，可用S3做底座；StarRocks也宣传对向量数据库（AI检索）有支持。Doris在3.0也实现了计算存储解耦和规划向量索引。</p>
</li>
<li><p><strong>易用性</strong>：StarRocks 延续了 Doris 易用的传统，甚至在UI、工具上做得更多。StarRocks 提供了<strong>可视化的Doris Manager</strong>（由SelectDB改进）、更自动的调优。因此对于新用户来说，StarRocks上手也很容易。Doris 在社区力量下也不断改进管理工具，如支持Prometheus监控，提供安装脚本等。两者运维复杂度相近，都比CH简单。StarRocks版本更新频率高，意味着新功能快但也可能不够稳定；Doris 相对慎重，企业用户或许更看重稳定性。</p>
</li>
<li><p><strong>社区与授权</strong>：Doris 是Apache社区，StarRocks由公司主导。Doris 开放贡献氛围好，而StarRocks核心开发以员工为主，外部贡献相对少。StarRocks早期因采用 Business License 一度引争议，但现已回归Apache协议，所以两者都是开源。但Doris对很多厂商更中立，StarRocks背后公司（现更名SelectDB）商业色彩强。客户在选型时，有时考虑社区活跃度和供应商绑定风险。Doris社区目前贡献者数、企业用户量更大一些。</p>
</li>
<li><p><strong>结论</strong>：StarRocks 打出的口号是“更快、更灵活”，确实在性能优化上做了许多投入，也支持更多前沿需求如向量检索。但Apache Doris 拥有广泛社区支持，功能上也几乎不落后。可说StarRocks一度“青出于蓝”，但Doris后来者居上追赶，双方如今性能差距不明显，架构上趋同。同质化下企业可根据<strong>信任度</strong>和<strong>支持</strong>选择：喜欢开源社区氛围和Apache品牌，可选Doris；看重原班人马商业支持或个性新功能，可考虑StarRocks。值得欣喜的是，这种竞争促进了双方快速演进，对用户来说都是利好。</p>
</li>
</ul>
<h3 id="8-3-Doris-vs-Greenplum"><a href="#8-3-Doris-vs-Greenplum" class="headerlink" title="8.3 Doris vs Greenplum"></a>8.3 Doris vs Greenplum</h3><p><strong>Greenplum</strong> 是老牌MPP数据仓库，基于PostgreSQL扩展。与Doris相比：</p>
<ul>
<li><p><strong>架构</strong>：GP 是典型 <strong>Shared Nothing MPP</strong>，由一主节点 + 多Segment节点组成。主节点负责SQL解析规划，Segment存数据并执行，架构形式上类似Doris FE/BE。但Greenplum的主节点存元数据和协调任务类似Doris FE；Segment执行类似BE。不同的是GP早期版本依赖文件共享（Mirrors）实现HA，不如Doris多副本那么无感。而且GP扩展需要重新均衡数据，成本很高。Doris 扩容更灵活，GP扩容可能得dump数据再导入。</p>
</li>
<li><p><strong>存储</strong>：GP 存储是 <strong>行存</strong>（后来也支持列存表，但不及现代列库优化）。GP的行存导致在大扫描时IO和解压不如Doris列存高效。GP通过对表建索引（B树）可优化点查，但面对海量数据分析其表索引作用有限反而增加维护成本。Doris 使用列存+多索引，针对分析性能优化更多。</p>
</li>
<li><p><strong>SQL 支持</strong>：GP 继承 PostgreSQL，SQL特性非常完整，复杂查询（嵌套子查询、递归CTE等）都支持良好。这是Doris略逊的地方（Doris在逐步完善，但GP多年积累全面SQL）。GP也支持事务和强一致，适合需要复杂事务+分析混合的少数场景。但GP缺乏Doris那样的物化视图自动匹配和外表对接功能（GP有ext table但对接新兴数据湖不多）。因此GP在<strong>灵活性</strong>上很强，可以胜任几乎所有SQL逻辑，但<strong>性能</strong>在大数据时代跟不上需求。Doris 舍弃了一部分不常用SQL支持，专注分析场景，性能高很多。</p>
</li>
<li><p><strong>性能</strong>：公认GP在超大数据量下性能远不如新型列式引擎。测试显示在TPCH/TPCDS等基准上，GP用更多硬件也达不到Doris速度。Doris 官方报告中称其对TPC-DS的平均性能是Trino/Presto的3-5倍。而Trino性能又通常优于GP。所以GP性能大概是Doris的1/3或更差。当然GP可垂直扩展，用强服务器提升，但性价比低。Doris横向扩展更方便成本更低。Greenplum适合以前TPCH几TB时代，现在百TB级分析GP已经力不从心。这也是GP市场份额被新型OLAP瓜分的重要原因。</p>
</li>
<li><p><strong>易用性与生态</strong>：GP生态成熟度高，和各种BI、ETL、工具适配完善。Doris依赖MySQL协议，这方面也不差。但GP学习成本高，要会PostgreSQL的运维，还要懂GP的分布配置等。Doris易用性更好（两类节点，MySQL语法）。GP社区近年开发放缓（主要Pivotal和开源社区，贡献少），Doris社区新兴有活力。很多GP用户在考虑迁移到Doris/ClickHouse等以提升性能和降低开销。</p>
</li>
<li><p><strong>应用</strong>：GP 在传统大型企业DW中常见，用于企业报表、数据集市，很稳健。但实时和交互分析不是GP强项。Doris 则针对实时交互设计，可以承担GP+Kylin等组合的工作。本质上GP属于<strong>老一代</strong>，Doris<strong>新一代</strong>。如果企业已用GP，对复杂SQL依赖深，可以用Doris做加速查询层；新的项目更多直接选择Doris了。</p>
</li>
</ul>
<h3 id="8-4-Doris-vs-Presto-Trino"><a href="#8-4-Doris-vs-Presto-Trino" class="headerlink" title="8.4 Doris vs Presto/Trino"></a>8.4 Doris vs Presto/Trino</h3><p><strong>Presto/Trino</strong> 是分布式SQL查询引擎，没有持久存储。对比 Doris：</p>
<ul>
<li><p><strong>架构</strong>：Presto 由Coordinator和Workers组成，Coordinator解析SQL生成分布式计划，Workers从外部存储拉取数据计算。它不存储数据，典型场景是查询HDFS上的Hive表或S3上的数据湖。Doris 则自有存储引擎，也可当查询引擎使用。Presto查询时会扫描远端存储文件，因此性能受限于网络和远端格式。Doris 存储本地化，扫描速度极快。Presto扩展性好，可以连接多种数据源，Doris 3.0 通过Connector也拥有类似能力。但Doris同时提供存储更适合重复高频查询。</p>
</li>
<li><p><strong>性能</strong>：Presto擅长跨源 adhoc 查询，但对单个大型数据集的查询延迟较高，秒级查询较难实现。而 Doris 针对数据组织和向量化优化，其平均查询性能显著优于Presto。官方对TPC-H/DS的测试 Doris 平均快Trino 3-5倍。这在各公司实际反映中也类似：Presto查询10亿行数据常需数十秒，Doris能做到秒级甚至亚秒。Presto优势在于可以不用搬移数据，直接查询数据湖。但如果数据需要频繁查询，通常会导入Doris以获得更好交互速度。因此很多架构是“数据湖+Presto”用于低频查询，“数据导入Doris”用于高频BI查询。</p>
</li>
<li><p><strong>SQL功能</strong>：Trino对SQL标准支持很好（窗口函数、复杂subquery等应有尽有），而 Doris 也大部分支持。Trino甚至实现一些Doris没有的如approx_percentile等，但Doris有HLL等Presto没有的。Trino扩展性体现在可支持NoSQL查询等，通过Connector。Doris 3.0 通过Trino兼容框架，也获得了连接Delta/Kudu等能力。因此功能差距在缩小。Doris 的物化视图、索引等是Presto所没有的性能加速手段。Presto每次都全量扫描计算，不会缓存，也无索引，所以对重复查询效率低。Doris的cache和MV让重复查询几乎免费。这是两者架构差异带来的。</p>
</li>
<li><p><strong>易用性</strong>：Presto部署也不复杂，但需要依赖Hive Metastore等。Doris 部署稍复杂一点（需要分片概念），但运维差异不大。Presto省去了数据加载时间，这是优点，但当数据量太大时，这点优点压不过查询性能的缺点。</p>
</li>
<li><p><strong>应用</strong>：Presto/Trino定位是<strong>统一SQL查询</strong>引擎，适合临时、跨库分析，不适合频繁报表。Doris 定位<strong>实时数仓</strong>，适合高并发报表，必要时也能通过外表查询部分外部数据。二者关系其实是互补多于竞争：很多企业是湖存储+Presto+OLAP数据库组合，各取所长。Presto可以查询冰山上几百TB冷数据，Doris负责热数据和统计结果。在VeloxDB博客中，认为Doris提供<strong>更好的实时分析体验</strong>和<strong>湖查询加速</strong>，即可以部分替代Presto/Trino作为快很多倍的查询引擎。Trino也在不断优化，但受限于存算分离架构，单查询延迟上很难赶上有存储优化的Doris。</p>
</li>
</ul>
<p>综上，不同产品各有定位：ClickHouse追求极致性能但使用门槛较高，StarRocks与Doris一脉相承在商业化上走得更快，Greenplum功能全但性能不足时代潮流，Presto整合性强但交互性能一般。Apache Doris 则大体上<strong>平衡</strong>了性能、功能、易用性。京东实践结论里也说：“一站式分析选Doris，复杂场景定制选CH”。而StarRocks作为Doris的强力竞品，使Doris社区不断快速迭代，形成良性竞争。总的来看，Doris 在<strong>国产开源分析数据库</strong>中已占有一席之地，性能上可满足绝大多数实时数仓需求，综合体验上具有突出优势，这也是越来越多企业选型Doris的重要原因。</p>
<h2 id="9-企业级实践案例与应用场景"><a href="#9-企业级实践案例与应用场景" class="headerlink" title="9. 企业级实践案例与应用场景"></a>9. 企业级实践案例与应用场景</h2><p>Apache Doris 自开源以来，已在众多企业落地应用，覆盖互联网、金融、制造、电信等行业的各种实时分析场景。以下总结典型的应用场景和实践案例：</p>
<h3 id="9-1-典型应用场景"><a href="#9-1-典型应用场景" class="headerlink" title="9.1 典型应用场景"></a>9.1 典型应用场景</h3><ul>
<li><p><strong>实时报表和运营分析</strong>：这是 Doris 最初的核心场景。在互联网公司中，Doris 常用于构建实时运营数据大屏和报表系统。通过 Flink 等将业务日志、交易数据实时导入 Doris，相关部门可以随时查询最新的关键指标。比如在某短视频平台，Doris 支撑了<strong>用户增长和内容运营</strong>报表，提供分小时、分地域的用户活跃、留存、转化等指标的秒级查询，让运营人员及时掌握产品状况。又如电商场景， Doris 承载<strong>实时销售与库存看板</strong>，分析各品类销售额、库存周转、转化率等，辅助秒级决策。</p>
</li>
<li><p><strong>即席查询 (Ad-hoc Analysis)<strong>：数据分析师经常需要对海量数据做临时的多维度钻取分析。Doris 适合做</strong>交互式数据查询</strong>引擎，支持用户用 BI 工具或SQL客户端直接对明细数据进行切片、汇总和下钻，响应时间在亚秒到数秒之间。例如某社交应用利用 Doris 实现<strong>用户行为路径</strong>的即席查询：分析用户从登录到完成某操作的转化漏斗，筛选特定人群再看细节。Doris 的高并发和低延迟满足了分析师频繁操作、不确定查询模式的需求。</p>
</li>
<li><p><strong>用户画像与推荐</strong>：在用户画像和推荐场景，需要对用户属性标签进行多条件组合筛选（如找出25-30岁女性最近7天浏览过商品X且未购买的人群）。Doris 倒排索引非常适用于这类<strong>标签检索</strong>。某互联网金融公司采用 Doris 存储用户多维行为标签，通过倒排索引实现毫秒级人群筛选，然后将筛选结果用于个性化推荐或营销活动。相比以往用ElasticSearch或Graph引擎，Doris 提供了统一SQL接口和稳定性能，将<strong>画像分析</strong>和<strong>实时查询</strong>合二为一。</p>
</li>
<li><p><strong>实时风控与监控</strong>：金融行业和运营系统中常需要实时监控指标、检测异常。Doris 可用来构建<strong>实时监控平台</strong>，存储大量日志或指标时间序列，并提供秒级多维分析。比如支付平台通过 Doris 汇聚交易流水，实时监控交易失败率、延迟分布，如出现异常峰值可立即从 Doris 查询相关维度（地区、银行等），迅速定位问题。又如银行反欺诈，Doris 存储用户交易行为并对接规则引擎，实时筛查可疑模式。高并发查询能力保证监控告警系统多指标同时分析时仍保持低延迟。</p>
</li>
<li><p><strong>统一数据服务（即席 OLAP 服务）</strong>：一些企业将 Doris 作为<strong>数据中台</strong>的即席查询服务。不同业务线的数据经过数据湖沉淀，部分汇总入 Doris，提供一个统一SQL接口给上层应用或自助分析使用。例如某制造企业搭建 Doris 来整合生产线传感器数据、质量检测数据、供应链数据，用于生产效率分析和异常追踪。各部门通过 Doris 提供的API或SQL访问获取自己需要的指标报表。这种场景下 Doris 扮演着<strong>数据超市</strong>角色，支撑全公司各类数据查询。</p>
</li>
<li><p><strong>湖仓一体分析加速</strong>：对于已经有海量数据湖（Hive/Iceberg）的企业，Doris 经常用来做<strong>数据湖查询加速</strong>。把湖中经常查询的部分数据导入 Doris 或通过外部表由 Doris 查询，以大幅降低查询时延。例如电信行业有大量话单数据存储在HDFS，通过 Doris 外表技术，联邦查询 Hive 分区并将热点数据缓存至 Doris 本地，实现10倍以上的查询提速。湖仓一体场景下 Doris 既可以加速<strong>历史数据</strong>的查询（通过外表/insert select），又能承载<strong>增量实时数据</strong>，构建一个统一查询层。这减少了数据重复存储和跨系统代价，越来越受到数据架构师青睐。</p>
</li>
</ul>
<h3 id="9-2-企业实践案例"><a href="#9-2-企业实践案例" class="headerlink" title="9.2 企业实践案例"></a>9.2 企业实践案例</h3><ul>
<li><p>**字节跳动 (TikTok)**：字节在其广告和内容推荐业务中广泛使用 Doris。据介绍，字节某广告业务实时数仓采用 Doris 存储了上PB级别的数据，每日增量数十TB，但仍然实现了秒级多维分析。该 Doris 集群服务于广告主的报表查询、内部运营分析，支持上千并发查询，涵盖地域、年龄、兴趣等几十维度。通过 Doris 的物化视图功能，字节构建了一系列预聚合表，极大加速了固定报表的响应速度，同时对临时查询也保持良好性能。高峰期 Doris 承载了数百万级QPS的点查询（如实时消耗、点击数API查询），利用RowCache等优化将平均延迟控制在几十毫秒。据报道，字节系产品（TikTok、今日头条等）已将很多内部分析场景统一到 Doris 平台上，取得了降本增效成果。</p>
</li>
<li><p><strong>百度</strong>：作为 Doris 的发起者，百度在自家大量使用 Doris（之前称Palo）。百度商业广告报表曾是 Doris 首个落地场景，解决了每天数十亿曝光点击日志的实时统计需求。Doris 支撑百度广告客户实时查询广告投放效果，原先需要10分钟才能统计的数据，换用 Doris 后几秒即可展现。目前百度在用户画像、人群分析、搜索日志分析等方面也使用 Doris 构建交互式分析平台，提高了运维效率（不再依赖繁琐的Hive离线跑批）。百度内部还基于 Doris 开发了自助分析工具，让业务方可以直接对接 Doris 写SQL完成数据探索，不需要数据团队介入。Doris 的稳定性在百度大规模应用中也经受住考验，比如发生节点故障自动切换毫无感知，极大减少集群运维量。</p>
</li>
<li><p><strong>腾讯</strong>：腾讯多个业务线采用 Doris 替换原有方案，如腾讯课堂的实时数仓、腾讯游戏的数据分析平台等。以腾讯课堂为例，他们将海量学习行为日志写入 Doris，给老师和运营提供实时的课堂参与度、习题正确率等报表。此前使用Spark+Kylin组合难以满足临近实时，现在 Doris 让课程数据可在分钟内更新呈现。腾讯游戏部门也用 Doris 实时监控游戏在线人数、付费转化等，在大型活动期间Doris承载上万TPS查询依然表现稳定。腾讯云更是将 Doris 打包为云上OLAP产品（云数据仓库CynosDB Doris版），服务于各行业客户。腾讯调研认为 Doris 在多维分析的吞吐和响应上都优于传统MPP数据库，非常适合用作交互分析引擎。</p>
</li>
<li><p><strong>金融行业</strong>：某大型银行引入 Doris 构建统一数据分析平台，用于实时风控和客户360视图。该行每天数亿交易流水、账务数据经CDC入湖，然后由 Doris 外表联合湖内历史数据+当天增量数据进行统一查询，实现T+0监控。相比原Hadoop+Greenplum方案，查询性能提升了一个数量级，硬件投入却减少约50%。同时 Doris 易用的SQL接口降低了分析团队使用门槛，许多报表从之前需要数小时专业开发缩短为分钟级自助生成。另一保险公司使用 Doris 进行营销数据分析，将各种渠道（App、小程序、线下）的用户行为汇总，实现客户意向实时预测，让营销人员能随时筛选出高意向客户名单。Doris 的高并发支持确保上千坐席同时查询筛选也能快速返回结果，不错失营销时机。</p>
</li>
<li><p><strong>工业制造</strong>：某大型制造集团将生产设备IoT数据接入 Doris，搭建工业互联网分析平台。每条产线每秒上百条传感器读数传入 Doris，存储一段时间的明细并做聚合，供工艺工程师随时查询异常时段机器参数。Doris 强大的摄取和查询能力让他们做到对产线上的异常（如温度过高、振动异常）在60秒内发现并追溯原因（查询前后5分钟详细参数）。相比传统SCADA系统只能事后分析，Doris 辅助他们实现了准实时的工艺优化和设备预测性维护，大幅降低次品率和停机时间。这显示出 Doris 在IoT大数据分析方面的潜力。</p>
</li>
</ul>
<p>综合各行业案例，Apache Doris 通常扮演以下角色：</p>
<ul>
<li><strong>实时数据仓库</strong>：承载在线业务各类日志/业务数据，为报表、监控提供支撑，强调及时性和并发。</li>
<li><strong>交互式分析引擎</strong>：供分析师/算法工程师直接对大数据做探索查询，加速数据洞察。</li>
<li><strong>数据服务中台</strong>：作为统一查询层，屏蔽底层不同存储，给上层应用和服务提供统一、高性能的数据访问接口（通过REST API或JDBC）。</li>
<li><strong>数据湖加速层</strong>：存储和缓存部分湖中数据或索引信息，提高对海量历史数据的查询效率。</li>
</ul>
<h3 id="9-3-行业典型分析"><a href="#9-3-行业典型分析" class="headerlink" title="9.3 行业典型分析"></a>9.3 行业典型分析</h3><ul>
<li><p><strong>互联网/大数据行业</strong>：以BAT为代表，Doris 主要用于广告、推荐、用户增长等实时分析，解决高并发和低延迟难题，是这些公司实时计算架构的核心组成。比如头条系在增长分析、字节广告上都大量使用Doris。用户规模大、数据量大、查询频繁的典型互联网场景，Doris 基本成为标配。</p>
</li>
<li><p><strong>金融行业</strong>：银行证券等将 Doris 用于风险监控（反欺诈、反洗钱）、交易分析和客户分析，因为金融数据具有高实时、高价值的特点，需要可靠的实时分析能力。Doris 的事务一致性和HA符合金融对数据正确性的要求。一些银行也将Doris应用于监管报送的数据集市建设，提高查询效率满足监管要求。</p>
</li>
<li><p><strong>零售电商</strong>：大零售公司用 Doris 进行销售监控、库存分析、用户行为分析等。双十一大促期间，Doris 支撑实时战报，全网销售额、各品类销售排行实时滚动显示。电商也用 Doris 的物化视图预聚合长周期历史销售数据，供经营决策分析月度走势等。实时和历史融合分析是零售典型需求，Doris 湖仓一体特性恰好满足，许多电商开始以 Doris 替代传统数仓+Kafka+ES的复杂链路，统一技术栈降低成本。</p>
</li>
<li><p><strong>通讯和运营商</strong>：电信运营商将 Doris 应用于话单记录详单分析、基站流量监测等。海量通话/上网记录实时汇总，对应套餐计费、异常话务告警、用户行为分析等。之前这些任务由MPP数据库或Hadoop完成，难以实时。引入 Doris 后实现了分钟级分析每个基站流量饱和情况，及时调整资源。还有运营商用 Doris 做用户套餐使用情况360分析，结合CRM数据提高用户运营精准度。</p>
</li>
<li><p><strong>游戏行业</strong>：游戏公司用 Doris 做游戏在线指标监控、玩家行为分析。游戏需要对在线人数、服务器性能指标实时监控，Doris 作为中台将游戏日志汇聚，实现运营和技术人员对全局状况一目了然。玩家行为上，通过 Doris 即席查询，可以分析玩家在游戏中各阶段的转化率和留存，调整游戏难度和活动策略。很多中小游戏企业没有复杂大数据团队，Doris 极大降低了他们搭建数据分析平台的门槛。</p>
</li>
</ul>
<p>可以看出，Doris 的共性优势在于<strong>实时、多维、大规模、高并发</strong>，凡是这类需求明显的行业都能受益于 Doris。加之它的易用和开源，传统行业（制造、能源、医疗）也可以较低成本引入先进的数据分析能力，这扩展了 Doris 的受众面。通过这些案例经验，也帮助 Doris 社区不断优化，例如加强高可用支持满足金融企业需求、完善数据类型支持方便更多应用等。可以预见，Apache Doris 在未来将赢得更广泛的企业采用，逐步成为实时分析领域的<strong>事实标准</strong>之一。</p>
<h2 id="10-Doris-部署架构、高可用与运维"><a href="#10-Doris-部署架构、高可用与运维" class="headerlink" title="10. Doris 部署架构、高可用与运维"></a>10. Doris 部署架构、高可用与运维</h2><p>企业在生产环境部署 Apache Doris 时，需要考虑集群架构设计、容灾高可用方案以及监控告警体系。Doris 在这些方面提供了较完善的支持。</p>
<h3 id="10-1-部署架构与集群规划"><a href="#10-1-部署架构与集群规划" class="headerlink" title="10.1 部署架构与集群规划"></a>10.1 部署架构与集群规划</h3><p><strong>基本部署</strong>：Doris 集群由若干 FE 和 BE 进程组成。典型生产部署采用 <strong>3个 FE</strong> 节点（1 Master + 2 Follower/Observer）和若干 BE 节点。FE 节点建议部署在独立服务器上，避免与BE争抢资源。3个FE保证了元数据的高可用（满足Quorum）。其中2个为Master-Follower模式，第三个可配置为Observer以扩展查询吞吐。BE 节点数量根据数据量和并发要求，可从几台起步扩展到数百台。Doris 经过测试能支持上百BE（一些用户实践超过500节点），总数据容量达数十PB。FE扩展能力也很强，一般3-5个已足够支撑大量并发（TikTok使用8个FE Observer应对超级高QPS场景）。FE之间可通过DNS轮询或客户端配置实现访问负载均衡。</p>
<p><strong>数据分布与分片</strong>：Doris 表创建时需指定分区（如按日期）和分桶键（hash分桶），这些决定数据在BE间如何分布。企业应根据查询模式合理设计：例如常按日期筛选则建按天分区，大表避免单分区太大。分桶键选择查询Join或group by常用键，以利用colocate join和并行。多桶有利于并行，但过多桶也增加元数据开销，一般每BE每分区保持10~100个Tablet为宜。副本数常为3，分散于不同机架。为容灾，可使用 Doris 的<strong>分区级别复制</strong>：即各分区的副本可分布在不同机架/机房，实现机架级甚至机房级冗余。Doris FE 提供**标签 (Resource Tag)**功能，可将BE分组，比如标记一组BE存冷数据，查询带上tag则调度到相应BE，实现冷热隔离。</p>
<p><strong>计算存储分离模式</strong>：Doris 3.0 引入了<strong>存储计算解耦</strong>的新架构模式。在这种模式下，BE 分为计算BE和存储BE，数据持久化在远端对象存储（如 S3）或 HDFS，仅缓存热数据在计算节点。这种模式下可以弹性增加计算BE处理高峰，并大幅节省本地存储成本。然而 Doris 3.0 暂不支持两种模式混布，需要部署时选定用一体化还是解耦模式。计算-存储分离适合公有云或需要极致弹性的场景。多数企业当前仍采用存储计算一体（本地盘存储数据），因为性能最优且架构成熟。未来版本 Doris 可能允许混合模式或平滑迁移。</p>
<p><strong>多租户隔离</strong>：如需在一套集群服务多个业务租户，可使用 Doris 的<strong>Resource Tag</strong>和<strong>Workload Group</strong>结合。通过 Resource Tag 可以将部分BE标记给特定租户，创建租户专属数据库只能落在该Tag BE上，这样在物理上资源隔离。Workload Group 则逻辑上限流不同租户查询并发和资源。这种软硬结合方式实现多租户共享集群的隔离与弹性。在华为云FusionInsight等平台上，已经使用这种机制提供 Doris 多租户能力。</p>
<p><strong>容器化与云部署</strong>：越来越多企业在 Kubernetes 中部署 Doris。为此社区提供了<strong>Doris Operator</strong>，可一键创建 Doris 集群。Operator 会自动拉起FE/BE容器、设置服务发现、甚至支持水平扩展BE节点。腾讯云、华为云等都推出了云上Doris服务，将部署和运维复杂度进一步降低。在物理部署时，一般将FE放在高可靠节点（如双电源机器），BE节点根据性能需要选择配置（CPU核数、内存、SSD/NVMe盘等）。Doris 对IO要求高，推荐使用NVMe SSD做数据盘，以充分发挥列存顺序读性能。若采用HDD，需要通过增加Cache内存等方式补偿。</p>
<h3 id="10-2-容灾与高可用"><a href="#10-2-容灾与高可用" class="headerlink" title="10.2 容灾与高可用"></a>10.2 容灾与高可用</h3><p><strong>高可用 (HA)<strong>：Doris 通过FE多副本和BE多副本机制实现 HA。FE Master故障时Follower秒级接管；BE故障时副本可用保证查询不受影响。实践中，为防止Master FE单点，企业通常部署3个FE (1 Master, 2 Follower)，并使用Keepalived或VIP让客户端透明连接Master。FE Master选举依赖BDB JE日志复制，需要确保FE节点部署在低时延网络环境。FE Master切换对DML事务提交会有影响（短暂冻结），但查询可以自动路由Observer继续执行。对 BE 故障，Doris FE默认5秒检测不到心跳则将其标记down，调度其Tablet副本替补。只要每Tablet至少2副本在线，查询能顺利完成。若副本不足2可能查询返回错误或不完整。因此建议副本数&gt;=3来保证双故障容忍。Doris 提供</strong>自动重建副本</strong>功能：BE故障后，FE在其他正常BE上复制缺失Tablet的数据（从其他副本拉取），恢复副本完整。如果BE短暂宕机重启，其数据通常都还在，只是那段时间查询走其他副本，并不需要重建；如果BE彻底损坏，则重建会消耗一定时间（取决于数据量）。</p>
<p><strong>容灾</strong>：Doris 支持<strong>双集群灾备</strong>部署，即两套 Doris 集群跨机房部署，一主一备。主集群提供服务，并通过<strong>定期增量备份</strong>把数据同步到备集群。备集群平时只同步不对外服务，当主集群不可用时手动或自动切换域名指向备集群提供服务。Doris 本身暂未内置异步复制机制，只能依靠Backup/Restore来做快照传输。Backup可按分区增量进行，运营上可以将每日增量数据备份到远端对象存储，再由备集群定时Restore。这实现了<strong>异地几近实时</strong>的数据容灾（RPO取决于备份频率，比如1小时一次）。而RTO取决于切换流程，一般可做到分钟级。对于要求更高的金融场景，也有公司定制 Doris binlog 增量同步工具实现近实时双活，但这不是官方特性。目前 Doris 能保证<strong>同城机房级HA</strong>（通过多副本跨机架/机房部署），但是跨区域的异地灾备仍需基于备份方式。好在 Doris Backup能提供一致性快照，对容灾恢复很有帮助。</p>
<p><strong>运维保障</strong>：为提高 Doris 集群稳健性，还需做好以下：</p>
<ul>
<li><strong>定期元数据元快照</strong>：FE内置BDB JE会定期dump元数据日志，但建议也人工定期 (daily) 备份 FE meta目录，防极端情况元数据损坏。</li>
<li><strong>负载均衡</strong>：Doris 自动均衡Tablet副本，确保每台BE存储和负载相近。但在扩容或大数据导入后可手动触发 <code>BALANCE</code> 操作。长期观察Leader分布，FE也可动态调整Tablet Leader均衡BE间查询负载。</li>
<li><strong>压缩和清理</strong>：定期合并小文件， Doris Compaction 会自动做，但可关注Compaction残留。对过旧数据，可以利用 Doris 分区的冷热分层特性，将老分区设为冷（less replication or even unload），或者归档导出删除，以释放存储和提升性能。</li>
<li><strong>版本管理</strong>：Doris 升级时通常支持滚动升级（先升级FE然后BE逐个）。要注意FE Master与Follower版本兼容，官方建议一次升级整个集群版本以免不兼容。</li>
</ul>
<h3 id="10-3-监控与告警"><a href="#10-3-监控与告警" class="headerlink" title="10.3 监控与告警"></a>10.3 监控与告警</h3><p>建立完善的<strong>监控告警</strong>系统对 Doris 集群稳定运行至关重要。Doris 提供多维度监控指标，推荐使用 Prometheus + Grafana 方案：</p>
<ul>
<li><p><strong>Prometheus 集成</strong>：Doris FE 和 BE 都提供HTTP接口输出指标（prometheus格式）。FE默认端口8030 <code>/metrics</code>，BE默认端口8040 <code>/metrics</code>，可被Prometheus抓取。指标包括进程级（CPU、内存、GC）、节点级（IO、网络）、查询统计（QPS、延迟分位）、缓存命中率、Compaction进度、Routine Load状态等。用户可配置Prometheus jobs来抓取所有FE/BE的metrics。</p>
</li>
<li><p><strong>Grafana Dashboard</strong>：社区提供了 Doris 的 Grafana 仪表盘模板，包括集群概览、FE状况、BE状况、查询统计、导入导出监控等图表。常看的指标如：FE QPS与延迟、FE内存、Journal复制延迟；BE查询并发、Tablet数、磁盘使用、Compaction backlog、Cache hit率等。通过Grafana可以及时发现热点节点（某BE CPU明显更高）或负载不均衡情况，从而调整。</p>
</li>
<li><p><strong>告警</strong>：可针对关键指标设阈值告警。例如：</p>
<ul>
<li>FE Master 切换（告警提示避免频繁主从倒换影响服务）。</li>
<li>FE Journal复制落后（follower与master日志偏移差距大预警）。</li>
<li>BE节点Down或心跳延迟高（节点故障/网络问题）。</li>
<li>BE 内存使用 &gt; 90%（接近mem_limit，可能触发kill）。</li>
<li>查询99延迟 &gt; 某值持续一段时间（查询性能异常）。</li>
<li>Routine Load堆积或失败率高（数据导入故障）。</li>
<li>磁盘使用 &gt; 80%（存储快满，需要扩容或清理）。</li>
<li>Compaction队列长度或等待时间过长（可能需要增加compaction线程或合并配置）。</li>
</ul>
</li>
</ul>
<p>Prometheus可配置这些规则并通过Alertmanager发送通知（邮件、短信等）。对于租户隔离环境，华为FusionInsight等也提供了多租户告警指标，如不同Workload Group的内存使用超过quota会触发租户告警。</p>
<ul>
<li><p><strong>日志监控</strong>：Doris FE、BE都有日志文件，可配置ELK收集分析。尤其FE的<strong>audit.log</strong>记录每条查询的耗时、扫描行数、用户等，用于安全审计和性能分析。企业经常将audit日志送到ES或Hive做报表分析，以了解哪些SQL耗费资源、哪些用户频繁访问。FE的error.log、BE的be.out需要关注有无异常stacktrace，一旦出现段错误或严重报错及时处理。Doris 也有<strong>profile</strong>功能，可对慢SQL执行计划和耗时进行分析，这对SQL优化很有帮助。</p>
</li>
<li><p><strong>Doris Manager</strong>：SelectDB和开源社区提供的 DorisManager 工具集成了监控、告警、运维管理等功能。通过Agent收集各节点信息上报，UI界面展示集群状态并可配置告警策略。这对不想自建Prometheus的用户是个选择。目前数千家企业在使用 Doris Manager。不过Doris Manager并非Apache项目，可选用。</p>
</li>
</ul>
<p>完善的监控和告警让 Doris 集群能实现<strong>7x24小时无人值守</strong>运行，一旦出现苗头问题提前告知，大大降低故障风险。很多公司将 Doris 监控纳入统一运维平台，如开放给巡检系统自动化检查compaction情况或定期备份校验结果。Doris 也提供了<strong>运维系统表</strong>（information_schema数据库）可查询集群元信息，如节点状态、正在执行的作业等。通过这些手段结合，运维人员可以及时了解集群健康并快速排查问题。</p>
<p>在实际经验中，Doris 部署运维的最佳实践包括：</p>
<ul>
<li>建立分层环境：开发/测试/生产隔离，多环境验证版本升级和参数调整效果。</li>
<li>参数调优：根据硬件和负载情况调整key参数，如 mem_limit（通常设物理内存的80%）、compaction_thread、tablet均衡策略、查询缓存开关等。一般Doris默认参数已较合理，仅在特殊场景需要调整。</li>
<li>安全：开启 Doris 用户权限管理，实现细粒度表/列访问控制。通过MySQL协议可使用LDAP等统一认证方式。网络上尽量使用内网隔离 Doris 集群，或者通过Proxy网关做访问控制。</li>
<li>文档：整理集群拓扑文档、数据分布规划、故障预案等，让团队对 Doris 有充分了解。定期演练Master FE故障切换、BE故障恢复流程，确保真正发生问题时能够按部就班解决。</li>
</ul>
<p>综上，Apache Doris 在企业生产运维方面已提供了<strong>全面的工具和机制</strong>。凭借多副本架构，它本身已经有较高的高可用能力；配合完善的监控告警和备份方案，可将风险降至最低。相比一些DIY的分析引擎方案（如Spark+ES组合），Doris 集群的运维显得更简单统一。很多企业在采用 Doris 后，反馈其维护成本比以往复杂的Lambda架构低了许多。这也是 Doris 成为企业选型偏好的重要因素：不仅性能强，还<strong>稳定可靠易维护</strong>，可以真正用于关键业务的生产系统。</p>
<h2 id="11-社区活跃度、版本演进与未来路线"><a href="#11-社区活跃度、版本演进与未来路线" class="headerlink" title="11. 社区活跃度、版本演进与未来路线"></a>11. 社区活跃度、版本演进与未来路线</h2><p>Apache Doris 作为开源项目，社区活跃度高，版本更新迅速，并有明确的未来发展规划。了解这些有助于企业评估 Doris 的生命力和长期演进方向。</p>
<h3 id="11-1-社区现状与活跃度"><a href="#11-1-社区现状与活跃度" class="headerlink" title="11.1 社区现状与活跃度"></a>11.1 社区现状与活跃度</h3><p>Doris 社区自2018年进入 Apache 孵化后发展迅猛，于2022年顺利毕业成为顶级项目。目前 Doris 拥有一个全球性社区，但主要贡献者和用户集中在中国互联网及大数据领域。到2024年，Doris 社区已聚集了<strong>600+贡献者</strong>，每月活跃贡献者超过120人。这在Apache项目中属于非常活跃的级别。GitHub上 Doris （apache/doris）仓库 Star 数持续上升（现约1.8k+），Issue和PR讨论频繁。近一年每月都有几十个 PR 合入，涵盖新功能开发、性能优化、BUG修复等。从社区邮件列表和Git提交看，来自腾讯、阿里、字节、京东、Airbnb 等公司员工的贡献都有，具有多元性，不依赖单一厂商。</p>
<p><strong>社区活动</strong>也丰富：每年举办 Doris Meetup、大数据大会专场分享，在中国和海外都推广项目。2023年举办了“Apache Doris Summit Asia 2023”，吸引了上千开发者线上线下参与，会上发布了 Doris 2025路线展望。社区还定期组织线上分享、性能挑战赛等，加速项目优化。Doris 社区还与兄弟项目互动，如Trino社区合作Connector框架，Arrow项目协作Flight接口等，体现开放共赢精神。</p>
<p><strong>商业生态</strong>：出现了一些围绕 Doris 的创业公司和云服务：如国内的 ArteryData (巨杉)、SelectDB (StarRocks商用版)、X-Margin (VeloxDB) 等提供 Doris 云服务或改进发行版。这些商业力量也在反哺开源，比如VeloxDB去年向 Doris 提交了 decoupling 存储的方案。目前上千家企业用户成为 Doris 实践者，许多在社区分享用例，形成良性循环。</p>
<h3 id="11-2-版本演进里程碑"><a href="#11-2-版本演进里程碑" class="headerlink" title="11.2 版本演进里程碑"></a>11.2 版本演进里程碑</h3><p>Doris 在过去几年快速迭代，路线清晰：</p>
<ul>
<li><p><strong>0.x 系列</strong>：2017开源后多次版本迭代完善基础功能。0.13/0.14 完成了存储引擎改造（Segment V2、向量化雏形）、0.15 支持MySQL Dump、0.15.1引入Unique Key初步支持等。</p>
</li>
<li><p><strong>1.x 系列</strong>：2021年发布 1.0 正式版，标志稳定可用里程碑。1.1版本引入<strong>向量化执行</strong>正式生效，大幅提升性能。1.1还增加了<strong>Java UDF</strong>扩展支持。1.2版本 (2022) 带来<strong>Unique Key MoW</strong> 和 <strong>异步物化视图</strong>等重大功能，极大拓展Doris应用场景。1.x系列奠定了Doris和StarRocks同步竞争的格局。</p>
</li>
<li><p><strong>2.0 版本</strong>：2022年底发布，包含<strong>Pipeline 执行引擎</strong>（实验性）、<strong>倒排索引</strong>、<strong>CBO优化</strong>、<strong>Partition Cache</strong>等革新。2.0将Pipeline默认关闭以稳妥推进。2.1版本 (2023年) 默认开启 Pipeline，引入<strong>Workload Group</strong>、<strong>Arrow Flight</strong>、<strong>更多函数类型</strong>等。2.x系列使 Doris 性能和功能全面追赶StarRocks。</p>
</li>
<li><p><strong>3.0 版本</strong>：2023年底发布的重大版本。3.0的主题是<strong>云原生和湖仓融合</strong>。它支持<strong>计算-存储分离模式</strong>、<strong>写入数据湖 (Hive/Iceberg)<strong>、</strong>Trino Connector</strong>框架。另外补齐<strong>半结构化数据类型</strong>(JSON/Variant)、<strong>向量化加载</strong>提升导入性能、<strong>二级索引</strong>完善(NGram)。3.0标志 Doris 转型为“实时湖仓”平台。3.x也在系统稳定性方面加强，如内存管理Spill完善、FE UI界面等改进。</p>
</li>
<li><p><strong>未来版本</strong>：预计4.0及以后的版本，会继续围绕 <strong>实时 (Real-time)<strong>、</strong>统一 (Unified)<strong>、</strong>云原生 (Cloud-native)</strong> 三个方向演进。例如4.0可能实现:</p>
<ul>
<li><strong>向量检索</strong>：支持向量数据类型和ANN索引，用于AI嵌入检索。</li>
<li><strong>更智能优化</strong>：自动物化视图建议与管理、基于机器学习的自适应查询优化。</li>
<li><strong>流批一体</strong>：引入流计算框架直接处理数据（如内置Flink runner），实现真正统一的Lambda架构替代方案。</li>
<li><strong>数据湖更深整合</strong>：支持更多湖格式（Delta Lake、Hudi rewrite logs）、实现湖表的事务merge-on-read等。</li>
<li><strong>调度资源隔离</strong>：引入多Compute Group概念，更细粒度资源隔离，使不同业务负载在共享集群更安全运行。</li>
<li><strong>AI 与 LLAP</strong>：社区已提出在GenAI时代让 Doris 成为AI数据基础设施的一部分。如通过Flight与Python notebook集成、支持LlamaIndex这类工具直接查询Doris等。</li>
<li><strong>跨集群/多活</strong>：可能探索双集群实时同步方案（基于CDC），实现异地多活甚至全球部署统一查询，满足更严格容灾要求。</li>
<li><strong>SQL 完善</strong>：支持更多复杂SQL特性如递归CTE、MERGE语句等，争取在分析SQL上100%兼容ANSI标准。</li>
</ul>
</li>
</ul>
<p>社区在2025年路线讨论中强调要让 Doris 成为**“更快、更智能、更开放的下一代分析数据库”<strong>。具体要做的包括向量计算支持、半结构化性能提升、执行计划自适应优化以及进一步提升扩展性等。这与前述猜想一致。特别是希望抓住AI大模型浪潮，将 Doris 定位为</strong>面向AI时代的分析+搜索引擎**。VeloxDB也撰文称 Doris 正成为AI基础设施中<strong>最快的分析&amp;搜索数据库</strong>，强调其可同时支持结构化分析和语义向量检索。</p>
<h3 id="11-3-未来发展路线"><a href="#11-3-未来发展路线" class="headerlink" title="11.3 未来发展路线"></a>11.3 未来发展路线</h3><p>综上，Doris 的未来发展可以总结为：</p>
<ul>
<li><strong>实时性</strong>：进一步降低数据可见延迟（如引入源到Doris的CDC链路优化，支持亚秒级数据同步），提高并发处理能力（更好的调度、CPU亲和等）。</li>
<li><strong>统一性</strong>：整合更多数据源和数据类型，真正实现 “One interface to rule all data”。继续增强对数据湖、OLTP数据的查询性能，让 Doris 成为数据枢纽。</li>
<li><strong>云原生</strong>：完善计算/存储分离，实现按需弹性扩缩。优化资源调度配合K8s调度器。推出Serverless形态 Doris，让用户无需关心节点管理。</li>
<li><strong>智能自治</strong>：引入<strong>自适应优化</strong>和<strong>机器学习调优</strong>。比如自动选择物化视图维护、自动索引建议、根据负载动态调整参数，让数据库自我调节达到最佳性能。</li>
<li><strong>新兴需求</strong>：支持<strong>向量数据</strong>和<strong>AI结合</strong>。例如embedding类型，原生ANN搜索算子，让 Doris 不仅能做结构化OLAP，也能加速向量相似度搜索，在推荐/问答系统中胜任向量库角色。</li>
<li><strong>更高可扩展</strong>：官方表示 2025 年目标支持千节点级规模，并完善分布式容错能力。包括Segment级子查询调度优化、对大量小文件和分区的处理性能提升等，让 Doris 在超大规模下依然表现稳定。</li>
</ul>
<p>通过这些路线，我们看到 Doris 社区野心勃勃，致力于把 Doris 打造成<strong>新一代一体化分析平台</strong>。这与当前数据领域趋势吻合：融合实时+离线、融合OLAP+搜索、赋能AI应用。企业可期望 Doris 在未来几年持续进步，满足更多复杂场景。对当前用户而言，社区活跃意味着问题能及时响应，功能不断丰富。对潜在采用者而言，Doris 显示出强劲生命力和广阔前景，是一个<strong>值得长期投入</strong>的技术。毕竟拥有活跃社区和明晰路线的开源项目，才能在快速变革的数据技术浪潮中不断演化，保护用户的技术投资不落伍。Apache Doris 正在以开源的力量引领实时分析数据库的前沿，其发展值得持续关注。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9E%B6%E6%9E%84/" rel="tag"># 架构</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/07/23/control-theory/" rel="prev" title="控制论理论基础、发展历程与软件设计应用调研报告">
      <i class="fa fa-chevron-left"></i> 控制论理论基础、发展历程与软件设计应用调研报告
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/08/01/Do-not-lose-money/" rel="next" title="创业第一步：不要赔钱">
      创业第一步：不要赔钱 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Doris-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="nav-number">2.</span> <span class="nav-text">1. Doris 整体架构设计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Frontend-FE-%E5%89%8D%E7%AB%AF%E6%9E%B6%E6%9E%84"><span class="nav-number">2.1.</span> <span class="nav-text">1.1 Frontend (FE) 前端架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Backend-BE-%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84"><span class="nav-number">2.2.</span> <span class="nav-text">1.2 Backend (BE) 后端架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E4%B8%8E%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6"><span class="nav-number">2.3.</span> <span class="nav-text">1.3 任务调度与容错机制</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%84%E7%BB%87"><span class="nav-number">3.</span> <span class="nav-text">2. 列式存储结构与数据组织</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E6%95%B0%E6%8D%AE%E5%88%86%E7%89%87%E4%B8%8E%E7%9B%AE%E5%BD%95%E7%BB%84%E7%BB%87"><span class="nav-number">3.1.</span> <span class="nav-text">2.1 数据分片与目录组织</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E5%8E%8B%E7%BC%A9%E7%BC%96%E7%A0%81"><span class="nav-number">3.2.</span> <span class="nav-text">2.2 列式存储与压缩编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E5%A4%9A%E7%BA%A7%E7%B4%A2%E5%BC%95%E6%9C%BA%E5%88%B6"><span class="nav-number">3.3.</span> <span class="nav-text">2.3 多级索引机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5%E4%B8%8E%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86"><span class="nav-number">3.4.</span> <span class="nav-text">2.4 数据写入与版本管理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BB%E9%94%AE%E6%A8%A1%E5%9E%8B%E4%B8%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E6%9B%B4%E6%96%B0"><span class="nav-number">3.4.1.</span> <span class="nav-text">主键模型下的数据更新</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E"><span class="nav-number">4.</span> <span class="nav-text">3. 查询执行引擎</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 查询执行流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-MPP-%E5%B9%B6%E8%A1%8C%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%88%86%E5%8F%91"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 MPP 并行与数据分发</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E8%B0%93%E8%AF%8D%E4%B8%8B%E6%8E%A8%E4%B8%8E%E5%88%86%E5%8C%BA%E8%A3%81%E5%89%AA"><span class="nav-number">4.3.</span> <span class="nav-text">3.3 谓词下推与分区裁剪</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-%E5%90%91%E9%87%8F%E5%8C%96%E6%89%A7%E8%A1%8C%E4%B8%8E-Pipeline-%E5%BC%95%E6%93%8E"><span class="nav-number">4.4.</span> <span class="nav-text">3.4 向量化执行与 Pipeline 引擎</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%8E%E4%BD%8E%E5%BB%B6%E8%BF%9F%E4%BC%98%E5%8C%96"><span class="nav-number">4.5.</span> <span class="nav-text">3.5 高并发与低延迟优化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E4%B8%8E%E5%AF%BC%E5%87%BA%E6%9C%BA%E5%88%B6"><span class="nav-number">5.</span> <span class="nav-text">4. 数据导入与导出机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E5%AE%9E%E6%97%B6%E6%95%B0%E6%8D%AE%E5%86%99%E5%85%A5"><span class="nav-number">5.1.</span> <span class="nav-text">4.1 实时数据写入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E6%89%B9%E9%87%8F%E5%AF%BC%E5%85%A5%E7%A6%BB%E7%BA%BF%E6%95%B0%E6%8D%AE"><span class="nav-number">5.2.</span> <span class="nav-text">4.2 批量导入离线数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA%E6%9C%BA%E5%88%B6"><span class="nav-number">5.3.</span> <span class="nav-text">4.3 数据导出机制</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Doris-%E7%9A%84-SQL-%E6%94%AF%E6%8C%81%E4%B8%8E%E7%B4%A2%E5%BC%95%E7%89%B9%E6%80%A7"><span class="nav-number">6.</span> <span class="nav-text">5. Doris 的 SQL 支持与索引特性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-SQL-%E5%85%BC%E5%AE%B9%E6%80%A7%E4%B8%8E%E5%8A%9F%E8%83%BD"><span class="nav-number">6.1.</span> <span class="nav-text">5.1 SQL 兼容性与功能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-%E7%89%A9%E5%8C%96%E8%A7%86%E5%9B%BE-Materialized-View"><span class="nav-number">6.2.</span> <span class="nav-text">5.2 物化视图 (Materialized View)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95%E4%B8%8E%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2"><span class="nav-number">6.3.</span> <span class="nav-text">5.3 二级索引与全文检索</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E9%AB%98%E5%B9%B6%E5%8F%91%E4%BD%8E%E5%BB%B6%E8%BF%9F%E5%9C%BA%E6%99%AF%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5"><span class="nav-number">7.</span> <span class="nav-text">6. 高并发低延迟场景优化策略</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1"><span class="nav-number">7.1.</span> <span class="nav-text">6.1 缓存设计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-%E5%B9%B6%E5%8F%91%E8%B0%83%E5%BA%A6%E4%B8%8E%E5%B7%A5%E4%BD%9C%E8%B4%9F%E8%BD%BD%E7%AE%A1%E7%90%86"><span class="nav-number">7.2.</span> <span class="nav-text">6.2 并发调度与工作负载管理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E4%B8%8E%E6%BA%A2%E5%87%BA%E4%BF%9D%E6%8A%A4"><span class="nav-number">7.3.</span> <span class="nav-text">6.3 内存管理与溢出保护</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-%E4%B8%8E%E4%B8%BB%E6%B5%81%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%AF%B9%E6%8E%A5%E8%83%BD%E5%8A%9B"><span class="nav-number">8.</span> <span class="nav-text">7. 与主流生态系统的对接能力</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-Flink-%E9%9B%86%E6%88%90"><span class="nav-number">8.1.</span> <span class="nav-text">7.1 Flink 集成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-Spark-%E9%9B%86%E6%88%90"><span class="nav-number">8.2.</span> <span class="nav-text">7.2 Spark 集成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-Hive-%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B9%96%E9%9B%86%E6%88%90"><span class="nav-number">8.3.</span> <span class="nav-text">7.3 Hive 与数据湖集成</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-%E5%90%8C%E7%B1%BB%E4%BA%A7%E5%93%81%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90"><span class="nav-number">9.</span> <span class="nav-text">8. 同类产品对比分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-Doris-vs-ClickHouse"><span class="nav-number">9.1.</span> <span class="nav-text">8.1 Doris vs ClickHouse</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-Doris-vs-StarRocks"><span class="nav-number">9.2.</span> <span class="nav-text">8.2 Doris vs StarRocks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-Doris-vs-Greenplum"><span class="nav-number">9.3.</span> <span class="nav-text">8.3 Doris vs Greenplum</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-Doris-vs-Presto-Trino"><span class="nav-number">9.4.</span> <span class="nav-text">8.4 Doris vs Presto&#x2F;Trino</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-%E4%BC%81%E4%B8%9A%E7%BA%A7%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">10.</span> <span class="nav-text">9. 企业级实践案例与应用场景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-%E5%85%B8%E5%9E%8B%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">10.1.</span> <span class="nav-text">9.1 典型应用场景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-%E4%BC%81%E4%B8%9A%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B"><span class="nav-number">10.2.</span> <span class="nav-text">9.2 企业实践案例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-3-%E8%A1%8C%E4%B8%9A%E5%85%B8%E5%9E%8B%E5%88%86%E6%9E%90"><span class="nav-number">10.3.</span> <span class="nav-text">9.3 行业典型分析</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-Doris-%E9%83%A8%E7%BD%B2%E6%9E%B6%E6%9E%84%E3%80%81%E9%AB%98%E5%8F%AF%E7%94%A8%E4%B8%8E%E8%BF%90%E7%BB%B4"><span class="nav-number">11.</span> <span class="nav-text">10. Doris 部署架构、高可用与运维</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#10-1-%E9%83%A8%E7%BD%B2%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92"><span class="nav-number">11.1.</span> <span class="nav-text">10.1 部署架构与集群规划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-2-%E5%AE%B9%E7%81%BE%E4%B8%8E%E9%AB%98%E5%8F%AF%E7%94%A8"><span class="nav-number">11.2.</span> <span class="nav-text">10.2 容灾与高可用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-3-%E7%9B%91%E6%8E%A7%E4%B8%8E%E5%91%8A%E8%AD%A6"><span class="nav-number">11.3.</span> <span class="nav-text">10.3 监控与告警</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-%E7%A4%BE%E5%8C%BA%E6%B4%BB%E8%B7%83%E5%BA%A6%E3%80%81%E7%89%88%E6%9C%AC%E6%BC%94%E8%BF%9B%E4%B8%8E%E6%9C%AA%E6%9D%A5%E8%B7%AF%E7%BA%BF"><span class="nav-number">12.</span> <span class="nav-text">11. 社区活跃度、版本演进与未来路线</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#11-1-%E7%A4%BE%E5%8C%BA%E7%8E%B0%E7%8A%B6%E4%B8%8E%E6%B4%BB%E8%B7%83%E5%BA%A6"><span class="nav-number">12.1.</span> <span class="nav-text">11.1 社区现状与活跃度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-2-%E7%89%88%E6%9C%AC%E6%BC%94%E8%BF%9B%E9%87%8C%E7%A8%8B%E7%A2%91"><span class="nav-number">12.2.</span> <span class="nav-text">11.2 版本演进里程碑</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-3-%E6%9C%AA%E6%9D%A5%E5%8F%91%E5%B1%95%E8%B7%AF%E7%BA%BF"><span class="nav-number">12.3.</span> <span class="nav-text">11.3 未来发展路线</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">爱妙妙爱生活</p>
  <div class="site-description" itemprop="description">日拱一卒，功不唐捐</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">221</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/samz406" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;samz406" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:lilin@apache.org" title="E-Mail → mailto:lilin@apache.org" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">蜀ICP备2021016919号-1 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">爱妙妙爱生活</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
